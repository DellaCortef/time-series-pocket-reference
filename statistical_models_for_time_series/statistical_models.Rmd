---
title: "Statistical Models for Time Series"
author: "Della"
date: "2024-11-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Statistical Models for Time Series

We will map some linear statistical models to **time series**. These models are related to linear regression, but represent correlations between data points in the same **time series**, 
unlike standard methods applied to cross-sectional data, where each data point is assumed to be independent of the others in the sample. We will analyze the following models:
  - autoregressive models (AR), moving average models (MA) and models 
  autoregressive integrated moving average (ARIMA);
  - vector autoregression (VAR);
  - hierarchical models;
  
Traditionally, these models have been the driving force in forecasting **time series** and continue to be used in a wide range of situations, from academic research to modeling 
in various fields of acting


## Why Not Use Linear Regression
As a data analyst, chances are you are already familiar with *linear regressions*. 
If not, a *linear regression* assumes you has *independent and identically distributed data (iid)*. According to we studied previously, this does not occur with **series data temporal**. 
In them, nearby points in time are usually strongly correlated with each other. In reality, when there are no correlations temporal, **time series** data are hardly useful for 
traditional tasks, such as predicting the future or understanding dynamics temporal.

It is not uncommon for tutorials and books on **time series** to teach us undue impression that *linear regression* is not useful for **series temporal**. What makes students believe 
that *linear regressions* simple are not enough. But that's not how it works. The *regression ordinary least squares linear* can be applied to data **time series**, 
provided that the following conditions are met:
*Assumptions regarding the behavior of the **time series**:*
- the **time series** has a linear response to its predictors;
- no input variable is constant over time or perfectly correlated with another input variable. Thus, the traditional requirement of *linear regression* of independent variables if 
amplifies to consider the temporal dimension of the data.
*Assumptions regarding the error:*
- for each point in time, the expected value of the error given all 
explanatory variables for all time periods (step forward 
[forward] and step backward [backward]), is 0;
- error in any given time period does not correlate with entries in no time period in the past or future. Therefore, a graph of the autocorrelation function of the errors 
will not indicate no default;
- the error variance is independent of time;

If these assumptions are valid, *least squares regression ordinary* is an unbiased estimator of the coefficients given to the inputs, even for **time series** data. In this case, 
the variances of the sampled estimates have the same mathematical form for the *regression linear* pattern. Therefore, if your data meets the assumptions listed, we can apply a 
*linear regression* that will undoubtedly help to offer clear and simple instructions on the behavior of the **time series**. The data requirements just described are similar to those 
in *standard linear regression* applied to cross-sectional data.Consequences of using *linear regression* outside the standards:
- coefficients will not minimize the model error;
- *p-values* to determine if your coefficients are non-zero will be incorrect as they are based on assumptions that are not answered. This means that your assessments of the significance of the coefficient may be wrong. Linear regressions serve to enable simplification and transparency when appropriate, but an incorrect model is not transparent at all.

The importance of following the assumptions of a model depends largely on the domain in question. Sometimes a model is applied with full knowledge that the basic assumptions are not met, as the consequences justify the benefits. For example, in high frequency trading, linear models are quite popular for many reasons despite no one believes that the data strictly follows all default assumptions.