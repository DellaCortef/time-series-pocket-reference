---
title: "Statistical Models for Time Series"
author: "Della"
date: "2024-11-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Statistical Models for Time Series

We will map some linear statistical models to **time series**. These models are related to linear regression, but represent correlations between data points in the same **time series**, 
unlike standard methods applied to cross-sectional data, where each data point is assumed to be independent of the others in the sample. We will analyze the following models:
  - autoregressive models (AR), moving average models (MA) and models 
  autoregressive integrated moving average (ARIMA);
  - vector autoregression (VAR);
  - hierarchical models;
  
Traditionally, these models have been the driving force in forecasting **time series** and continue to be used in a wide range of situations, from academic research to modeling 
in various fields of acting


## Why Not Use Linear Regression
As a data analyst, chances are you are already familiar with *linear regressions*. 
If not, a *linear regression* assumes you has *independent and identically distributed data (iid)*. According to we studied previously, this does not occur with **series data temporal**. 
In them, nearby points in time are usually strongly correlated with each other. In reality, when there are no correlations temporal, **time series** data are hardly useful for 
traditional tasks, such as predicting the future or understanding dynamics temporal.

It is not uncommon for tutorials and books on **time series** to teach us undue impression that *linear regression* is not useful for **series temporal**. What makes students believe 
that *linear regressions* simple are not enough. But that's not how it works. The *regression ordinary least squares linear* can be applied to data **time series**, 
provided that the following conditions are met:
*Assumptions regarding the behavior of the **time series**:*
- the **time series** has a linear response to its predictors;
- no input variable is constant over time or perfectly correlated with another input variable. Thus, the traditional requirement of *linear regression* of independent variables if 
amplifies to consider the temporal dimension of the data.
*Assumptions regarding the error:*
- for each point in time, the expected value of the error given all 
explanatory variables for all time periods (step forward 
[forward] and step backward [backward]), is 0;
- error in any given time period does not correlate with entries in no time period in the past or future. Therefore, a graph of the autocorrelation function of the errors 
will not indicate no default;
- the error variance is independent of time;

If these assumptions are valid, *least squares regression ordinary* is an unbiased estimator of the coefficients given to the inputs, even for **time series** data. In this case, 
the variances of the sampled estimates have the same mathematical form for the *regression linear* pattern. Therefore, if your data meets the assumptions listed, we can apply a 
*linear regression* that will undoubtedly help to offer clear and simple instructions on the behavior of the **time series**. The data requirements just described are similar to those 
in *standard linear regression* applied to cross-sectional data.Consequences of using *linear regression* outside the standards:
- coefficients will not minimize the model error;
- *p-values* to determine if your coefficients are non-zero will be incorrect as they are based on assumptions that are not answered. This means that your assessments of the 
significance of the coefficient may be wrong. Linear regressions serve to enable simplification and transparency when appropriate, but an incorrect model is not transparent at all.

The importance of following the assumptions of a model depends largely on the domain in question. Sometimes a model is applied with full knowledge that the basic assumptions are not 
met, as the consequences justify the benefits. For example, in high frequency trading, linear models are quite popular for many reasons despite no one believes that the data 
strictly follows all default assumptions.


## Statistical Methods Developed for Time Series

We will analyze statistical methods developed exclusively for data of **time series**. First, we will study the methods developed for **time series** *univariate* data, starting with 
a simple autoregressive model that states that the future values of a **time series** are a function of their past values. Afterwards, we will move on to few for more complex models, 
and we will conclude our study with a analysis on vector autoregression for **time series** *multivariate* and some specialized **time series** methods, such as *GARCH* models and 
hierarchical modeling.

### Autoregressive Models

The autoregressive (AR) model is based on the intuition that the past predicts the future. In this way, they presuppose a **series process temporal** in which the value at a point in 
time *t* is a function of the series values at previous points in time.

And so that we have an idea of how statisticians use these methods and its properties, our analysis will be thorough. Therefore, we will start with a comprehensive theoretical overview. If you 
are not interested in technicality of how the properties of statistical models are calculated in **time series**, just take a look at this section.<br><br>

#### Using algebra to understand constraints in AR processes

Autoregression resembles what many people would use as their first attempt to adjust a **time series**, especially if you do not have no information other than the **time series** 
itself. It's exactly what its name says: a regression on past values to predict values futures. The simplest AR model is represented like this:

y<sub>(1)</sub> = b<sub>(0)</sub> + b<sub>(1)</sub> * y<sub>(t-1)</sub> + e<sub>(t)</sub>
 
- The value of the series at time *t* is a function of a constant *b<sub>(0)</sub>*, its value in the previous time interval multiplied by another constant 
*b<sub>(1)</sub>* * *y<sub>(t-1)</sub>* and an error term that also varies with time *e<sub>(t)</sub>*. It is assumed that this error term has a variance constant and a mean of 0. 
We represent an autoregressive term as a AR(1) model that considers the past only in the immediately previous moment, as it includes a lag lookback. By the way, the AR(1) model is 
identical in form to a simple *linear regression* model with just one explanatory variable. That is, it maps: y<sub>(1)</sub> = b<sub>(0)</sub> + b<sub>(1)</sub> * x + e
- We can calculate the expected value of y<sub>(1)</sub> and its variance given y<sub>(t-1)</sub> if we know the value b<sub>(0)</sub> and b<sub>(1)</sub>:

*E(y<sub>(t)</sub>|y<sub>(t-1)</sub>) = b<sub>(0)</sub> + b<sub>(1)</sub> x y<sub>(t-1)</sub> + e<sub>(t)</sub>*

*Var(y<sub>(t)</sub>y<sub>(t-1)</sub>) = Var(e<sub>(t)</sub>) = Var(e)*

- The generalization of the previous notation allows the present value of an AR process to depend on the most recent values *p*, generating a process AR(*p*). Now we switch to a more 
traditional notation, which uses Ø to represent autoregression coefficients:
*y<sub>(t)</sub> = Ø + Ø<sub>(1)</sub> x y<sub>(t-1)</sub> + Ø<sub>(2)</sub> x y<sub>(t-2)</sub> + ... + + Ø<sub>(p)</sub> x y<sub>(t-p)</sub> + e<sub>(t)</sub>*

As we analyzed previously, stationarity is a fundamental concept in the analysis of **time series**, as it is the requirement of many models, including AR models. We can determine the 
conditions for an AR model to be stationary from the definition of stationarity:

*y<sub>(t)</sub> = Ø<sub>(0)</sub> + Ø<sub>(1)</sub> x y<sub>(t-1)</sub> + e<sub>(t)</sub>*

We assume that the process is stationary. So we work “one step backward” to see how this impacts the coefficients. First, based on the assumption of stationarity, we know that the 
expected value of the process must be the same at all times. We can rewrite y<sub>(t)</sub>:

*E(y<sub>(t)</sub>) = μ = E(y<sub>(t-1)</sub>)*

By definition, *e<sub>(t)</sub>* has an expected value of 0. Furthermore, the phis (φ) are constants, so their expected values are their constant values.

*E(y<sub>(t)</sub>) = E(Ø<sub>(0)</sub> + Ø<sub>(1)</sub> x y<sub>(t-1)</sub> + e<sub>(t)</sub>)*  
*E(y<sub>(t)</sub>) = μ*  
*Ø<sub>(0)</sub> = Ø<sub>(1)</sub> x μ + 0*  <br><br>
Simplified to:  
*μ = Ø<sub>(0)</sub> = Ø<sub>(1)</sub> μ*

Thus, we find a relationship between the process mean and the underlying AR(1) coefficients. <br><br>
We can follow a similar approach to see how constant variance and covariance impose conditions on the Ø coefficients. Replacing the value of Ø<sub>(0)</sub>, deriving the 
previous equation:

*Ø<sub>0</sub> = μ x (1 - Ø<sub>1</sub>)*<br>
*y<sub>t</sub> = Ø<sub>0</sub> + Ø<sub>1</sub> x y<sub>(t-1)</sub> + e<sub>t</sub>*<br>
*y<sub>t</sub> = (μ = μ x Ø<sub>1</sub>) + Ø<sub>1</sub> x y<sub>(t-1)</sub> + e<sub>t</sub>*<br>
*y<sub>t</sub> - μ = Ø<sub>1</sub>(y<sub>(t-1)</sub> - μ)+ e<sub>t</sub>*<br>

Since this **time series** is stationary, we know that the calculation at time *t*-1 must be the same as that at time *t*:

*y<sub>(t-1)</sub> - μ = Ø<sub>1</sub>(y<sub>(t-2)</sub> - μ) + e<sub>(t-1)</sub>*

Then we can replace:

*y<sub>t</sub> - μ = Ø<sub>1</sub>(Ø<sub>1</sub>(y<sub>(t-2)</sub> - μ) + e<sub>(t-1)</sub>) + e<sub>t</sub>*

Rearranging:

*y<sub>t</sub> - μ = e<sub>t</sub> + Ø<sub>1</sub>(e<sub>(t-1)</sub> + Ø<sub>1</sub>(y<sub>(t-2)</sub> - μ))*

Instead of working on *y<sub>(t-1)</sub>*, we will work on *y<sub>(t-2)</sub>*:

*y<sub>t</sub> - μ = e<sub>t</sub> + Ø<sub>1</sub>(e<sub>(t-1)</sub> + Ø<sub>1</sub>(e<sub>(t-2)</sub> + Ø<sub>1</sub>(y<sub>(t-3)</sub> - μ))) = e<sub>t</sub> + Ø x e<sub>(t-1)</sub> + $Ø^2$ x e<sub>(t-2)</sub> + $Ø^3$ x e<sub>(t-3)</sub>*<br><br>


With this, we can conclude more generally that  **y<sub>t</sub> - μ = $\sum_{i=1}^{\infty}$ Ø<sub>1</sub>$^i$ x e<sub>(t-1)</sub>**<br><br>


This result can then be used to calculate the expected *E[(y<sub>t</sub> - μ) x e<sub>(t+1)</sub>] = 0*, given that the values of *e<sub>t+1</sub>* at different values of *t* they are 
independent. From this, we can conclude that the covariance of *y<sub>(t-1)</sub>* e *e<sub>t</sub>* is 0, as it should be. We can use similar logic to calculate the variance of 
*y<sub>t</sub>* squaring this equation:

*y<sub>t</sub> - μ = Ø<sub>1</sub>(y<sub>(t-1)</sub> - μ + e<sub>t</sub>*

*var(y<sub>t</sub>) = Ø<sub>1</sub>$^2$ var(y<sub>(t-1)</sub>) + var(e<sub>t</sub>)*

Since the variance quantities on each side of the equation must be equal due to stationarity, this means:

*var(y<sub>t</sub>) = $\frac{\text{var}(e_t)}{1 - \phi_1^2}$*

Considering that the variance must be greater than or equal to 0 by definition, we can see that *Ø<sub>1</sub>$^2$* must be less than 1 to guarantee a positive value on the right side 
of the previous equation. that is, for a stationary process, it must have -1 < *Ø<sub>1</sub>* < 1. This is a necessary and sufficient condition for this type of weak stationarity.


#### Choosing parameters for an AR (p) model

To evaluate the suitability of an AR model on the data, we will start by plotting the process and its *partial autocorrelation function (PACF)*. The PACF of an AR process must be 
reduced to zero beyond the order *p* of an AR process (*p*), providing concrete and visual indications of the order of an AR process seen empirically in the data.

In contrast, an AR process will not have an informative autocorrelation function (ACF), although it will have the characteristic shape of an ACF: exponential tapering with increasing 
time offset.

Let's analyze this with some real data. We will use demand forecast data published by the **UCI Machine Learning Repository** (*https://perma.cc/B7EQ-DNLU*). First, we will 
plot the data in chronological order:

![Alt text](https://github.com/DellaCortef/time-series-pocket-reference/blob/main/statistical_models_for_time_series/images/pacf-banking-orders-2-demand.R.png?raw=true/pacf-banking-orders-2-demand.R.png)
We can observe that the PACF value exceeds the significance threshold of 5% in lag 3. This is compatible with the results of the *ar()* function available in the **stats** package of 
**R**. The *ar()* function automatically chooses the order of an autoregressive model if none is specified:

---
title: "AR Model Output"
output: html_document
---

##### AR Model Results

##### - Call
`ar(x = demand_banking, method = "mle")`

##### - ar() Coefficients
```{r, echo=FALSE}
# Define the coefficients in a data frame
coefficients <- data.frame(
  Order = 1:3,
  Coefficient = c(-0.1360, -0.2014, -0.3175)
)

# Display the table with knitr::kable
knitr::kable(coefficients, col.names = c("Order", "Coefficient"))
```

If we look at the documentation for the *ar()* function (*https://perma.cc/8H8Z-CX9R*), we can see that the selected order is determined (with default parameters that we do not change) 
it shows that the visual selection we made examining PACF is compatible with the selection that would be made minimizing an information criterion. Although there are two different ways 
of choosing the model order, here they are compatible.

Note that the *ar()* function also provided us with the coefficients for the model. However, we can restrict the coefficients. For example, when analyzing the PACF, we may ask ourselves 
whether we want to include a coefficient for the lag -1 term or whether we should assign that term a mandatory coefficient of 0, given that its PACF value is well below the threshold 
used for significance. In this case, we can use the *arima()* function, also from the **stats** package.

Next, we will demonstrate how to call the function to adjust an AR (3) by setting the ordering parameter to c(3, 0, 0), where 3 refers to the order of the AR component:
<br><br>

---
title: "ARIMA Model Output"
output: html_document
---

##### ARIMA Model Results

##### - Call
`arima(x = demand_banking, order = c(3, 0, 0))`

##### - arima() Coefficients Table

```{r, echo=FALSE}
# Define the coefficients and standard errors in a data frame
coefficients <- data.frame(
  Term = c("ar1", "ar2", "ar3", "intercept"),
  Estimate = c(-0.1358, -0.2013, -0.3176, 79075.350),
  `Std. Error` = c(0.1299, 0.1289, 0.1296, 2981.124)
)

# Display the table with knitr::kable
knitr::kable(coefficients, col.names = c("Term", "Estimate", "Std. Error"), caption = "ARIMA Model Coefficients")
```

To introduce knowledge into our model, we can constrain a coefficient to 0. For example, if we want to constrain lag -1 to 0 in our model, we will use the following call:
<br><br>

---
title: "ARIMA Model Output"
output: html_document
---

##### ARIMA Model Results

##### - Call
`arima(x = demand_banking, order = c(3, 0, 0), transform.pars = FALSE, fixed = c(0, NA, NA, NA))`

##### - arima() Coefficients Table

```{r, echo=FALSE}
# Create a data frame for coefficients and standard errors
coefficients <- data.frame(
  Term = c("ar1", "ar2", "ar3", "intercept"),
  Estimate = c(0, -0.1831, -0.3031, 79190.705),
  `Std. Error` = c(0, 0.1289, 0.1298, 3345.253)
)

# Display the table using knitr::kable
knitr::kable(coefficients, col.names = c("Term", "Estimate", "Std. Error"), caption = "ARIMA Model Coefficients")
```

We will now inspect the performance of our model on our training data in order to evaluate the goodness of fit of our model. We can do this in two ways:
- plotting the ACF of the residuals (of errors) to analyze whether there is an autocorrelation pattern that our model does not address;
- another test normally performed is the *Ljung-Box test*, a general test of the randomness of a **time series**;

First, we will plot the residuals ACF:

![Alt text](https://github.com/DellaCortef/time-series-pocket-reference/blob/main/statistical_models_for_time_series/images/acf-residuals-ar-adjusted.R.png?raw=true)

None of the ACF values exceed the significance threshold. Obviously we should not blindly give in to a significance threshold to evaluate or reject it, but this observation is a useful 
data point in a model that we believe is satisfactory for other reasons. We do not see a pattern of autocorrelation between the residuals (i.e., the error terms). If we had seen this 
pattern, we would likely return to our original model, considering the inclusion of additional terms to add complexity and explain the significant autocorrelation of the residuals.

The other commonly used test is *Ljung-Box test*, a general test of the randomness of a **time series**. In formal terms, this test lists the following null and alternative hypotheses:

- H0: the data do not show serial correlation;
- H1: the data show serial correlation;

This test is often applied to the AR model (and in general ARIMA), and, more specifically, to the model fit residuals, rather than the model itself:

---
title: "Box-Ljung Test Output"
output: html_document
---
<br><br>

##### Box-Ljung Test

##### - Test Results

The Box-Ljung test was conducted on the residuals.

- **Data**: `est.1$residuals`
- **X-squared**: 9.3261
- **Degrees of Freedom (df)**: 

We applied the Ljung-Box test to our *est.1* model to evaluate the goodness of fit. We cannot reject the null hypothesis that the data do not show serial correlation. This is 
confirmation of what we just discovered by plotting ACF of the residuals.
<br><br>

#### Forecasting with an AR process (*p*)

In the next developments, we will illustrate how to make predictions with AR processes. First, we will explore the one-step-ahead case, and then we will analyze how the 
multiple-step-ahead prediction differs from the other case. The good news is that little changes from a programming perspective, although the underlying math is more complicated in the 
latter case.

#### - one-step ahead forecast
- first, we will consider the case where we want to predict one step ahead with a known (or estimated) AR model. In this case, we have all the information we need;
- we will continue working with the model based on demand data, with the lag coefficient -1 restricted to 0 (set as *est.1*). We will plot the forecast using the *filtered()* function 
from the **forecast** package;

![Here we see the original **time series** on the solid line and, on the dashed line, the adjusted **time series**.](https://github.com/DellaCortef/time-series-pocket-reference/blob/main/statistical_models_for_time_series/images/forecast-one-step-ahead-est.1.Ra.png?raw=true)

<br><br>
Now we will think about the quality of the forecast. If we calculate the correlation between the predicted value and the actual value, we get 0.29. Depending on the context, this isn't 
so bad, but remember that sometimes differentiating the data ends up removing possible strong relationships and replacing them with one that is random. This can especially occur if the 
data is not actually stationary when we fit it, so that an unidentified trend can be masked as good model performance, when in fact it is just a quirk of the data that should be 
addressed before modeling.

<br><br>

#### - Multi-step ahead forecast
- to predict 2 steps ahead, for example, we would first need to predict one step ahead and then use the results to predict the next step forward;
- for our case, we do not need to implement this dynamic, since the value of *y<sub>(t-1)</sub>* is not used to predict *y<sub>(t+1)</sub >*;
- however, if we want to go further, we will need to generate predicted futures as inputs into our prediction. Let's predict *y<sub>(t+3)</sub>*. A model with dependent coefficients 
*y<sub>(t+1)</sub>* and y<sub>t</sub>* will be required. So we will need to predict these two values - *y<sub>(t+1)</sub>* and *y<sub>t</sub>* - and then use them in predicting y<sub> (t+3)</sub>. As before, we can use the *fitted()* function from the **forecast** package;

Using the *fitted()* function with an additional parameter *h* for the horizon. It is worth remembering that our object *est.1* represents an AR (3) process with lag coefficient -1 (time - 1) 
restricted to 0:

fitted(est.1, h = 3)
NULL

We can use the facility to predict many steps ahead, generating multiple predictions of several steps ahead for different horizons. In the next example, we can look at the variance of 
predictions made further and further into the distant future from the same underlying model.

As we can see in the next figure, the forecast variance decreases with the increase in the horizon ahead. The reason for this - which is a salient limitation of the model - is that the further 
we go in time, the less the actual data matters, as the coefficients for the input data only consider a finite prior set of time points. In other words, the most distant predictions in time 
converge to the unconditional prediction - that is, not conditioned on data. The future prediction approaches the mean value of the series as the time horizon increases, so the variance of both 
the error term and the predicted values decreases to 0, as the predicted values tend to converge to the unconditional mean value:

---
title: "Forecast Output"
output: html_document
---

##### Forecast Results

Here is the forecast output with the point estimates and confidence intervals.

```{r, echo=FALSE}
# Create a data frame for the forecast output
forecast_results <- data.frame(
  Step = 5:7,
  `Point Forecast` = c(0, 0, 0),
  `Lo 80` = c(-50743.49, -50743.49, -50743.49),
  `Hi 80` = c(50743.49, 50743.49, 50743.49),
  `Lo 95` = c(-77605.47, -77605.47, -77605.47),
  `Hi 95` = c(77605.47, 77605.47, 77605.47)
)

# Display the table
knitr::kable(forecast_results, caption = "Forecast Output with Confidence Intervals")
```
<br><br>


## Moving Average Models

A moving average (MA) model is based on a process in which the value at each point in time is a function of value "error" terms from the recent past, each independent of the others. Let's return 
to this model with the same steps we used to study AR models.


### The model

A moving average model can be expressed similarly to an autoregressive model, except that the terms included in the linear equation refer to present and past error terms rather than present and 
past values of the process itself. Thus, an MA model of order *q* is expressed as:

*y<sub>t</sub> = μ + e<sub>t</sub> + θ<sub>1</sub> X e<sub>(t-1)</sub> + θ<sub>2</sub> x e<sub>(t-2)</sub> + ... + θ<sub>q</sub> x e<sub>(t-q)</sub>*

Economists refer to these error terms as "shocks" to the system, while an electrical engineer might refer to them as "impulses" and the model itself as an infinite impulse response filter, 
meaning that the effects of any particular impulse last only a finite period of time. What is important is the concept that many independent events at different times in the past impact the 
current value of the process, in addition to the individual contribution of each of them.

##### - O Operador Backshift

The backshift operator, also known as the lag operator, operates on **time series** points and shifts them one step back each time 
it is applied. In general:
  
*B$^k$y<sub>t</sub> = y<sub>(t-k)</sub>*

The backshift operator helps simplify the expression of **time series** models. For example, an MA model can be rewritten as:
  
*y<sub>t</sub> = μ + (1 + θ<sub>1</sub> X B + θ<sub>2</sub> x B$^2$ + ... + θ<sub>q</sub> x B$^q$)e<sub>t</sub>*

By nature, MA models have weak stationarity without the need to impose any restrictions on their parameters. This is because the moving average and variance of an MA process are finite and time 
invariant, as the error terms are assumed to be iid with the mean 0. For example:

*E(y<sub>t</sub> = μ + e<sub>t</sub> + θ<sub>1</sub> X e<sub>(t-1)</sub> + θ<sub>2</sub> x e<sub>(t-2)</sub> + ... + θ<sub>q</sub> x e<sub>(t-q)</sub>)* = *E(μ) + θ<sub>1</sub> x 0 + θ<sub>2</sub> x 0 + ... = μ*

To calculate the variance of the process, we use the fact that the terms *e<sub>t</sub>* are iid, and also the general statistical property that the variance of the sum of two random variables 
is equal to their individual variances. added twice to their covariance. For iid variables, the covariance is 0. This generates the expression:

*Var(y<sub>t</sub>) = (1 + θ<sub>1</sub>$^2$ + θ<sub>2</sub>$^2$ + ... + θ<sub>q</sub>$^2$) x σ<sub>e</sub>$^2$*

Therefore, both the mean and variance of an MA process are constant over time, regardless of the parameter values.


#### Choosing the parameters for an MA (q) process

We will fit an MA model to the same data that we used to fit the AR model, and we can use ACF to determine the order of the MA process:

![ACF of MA process](https://github.com/DellaCortef/time-series-pocket-reference/blob/main/statistical_models_for_time_series/images/acf-ma-process.R.png?raw=true)
<br>

##### - ACF and PACF standards differ from MA and AR processes
Unlike an autoregressive process that has a slowly exponentially decaying ACF, the definition of the MA process guarantees a sharp cutoff of the ACF for any value greater than *q*, the order of 
the MA process. This is because an autoregressive process depends on previous terms, and they incorporate previous impulses into the system, whereas an MA model, which incorporates impulses 
directly through its value, has a mechanism to prevent the propagation of impulses from progressing indefinitely. .

We can observe significant values at lags 3 and 9, so we fit an MA model with these lags. Care must be taken not to unintentionally restrict the wrong coefficients to 0 in the model, which we 
can confirm by displaying:

---
title: "ARIMA Model Output"
output: html_document
---

##### ARIMA Model Results

##### - Call
`arima(x = demand_banking, order = c(0, 0, 9), fixed = c(0, 0, NA, rep(0, 5), NA, NA))`

##### Coefficients Table

```{r, echo=FALSE}
# Create a data frame for the coefficients and standard errors
coefficients <- data.frame(
  Term = c("ma1", "ma2", "ma3", "ma4", "ma5", "ma6", "ma7", "ma8", "ma9", "intercept"),
  Estimate = c(0, 0, -0.4725, 0, 0, 0, 0, 0, -0.0120, 79689.809),
  `Std. Error` = c(0, 0, 0.1459, 0, 0, 0, 0, 0, 0.1444, 2674.593)
)

# Display the table
knitr::kable(coefficients, col.names = c("Term", "Estimate", "Std. Error"), caption = "ARIMA Model Coefficients")
```


We should also check our fit, as we did in the AR model, by plotting the ACF of the model residuals and, with a second test of model performance, running the **Ljung-Box** test to check the 
overall randomness in any fit of the residuals. . Note that the *Box.test()* input requires us to specify the number of degrees of freedom - that is, how many model parameters were free to be 
estimated rather than being restricted to a specific value. In this case, the free parameters were the intercept, as well as the MA3 and MA9 terms:
<br><br>

---
title: "Box-Ljung Test Output"
output: html_document
---

##### Box-Ljung Test

##### - Test Results

- **Data**: `ma.est$residuals`
- **X-squared**: 7.6516
- **Degrees of Freedom (df)**: 7
- **p-value**: 0.3643

Não podemos rejeitar a hipótese nula de que não há correlação temporal entre os pontos residuais. Do mesmo modo, um gráfico ACF dos resíduos sugere nenhuma correlação temporal:

![ACF residuals MA](https://github.com/DellaCortef/time-series-pocket-reference/blob/main/statistical_models_for_time_series/images/acf-residuals-ar-adjusted.R.png?raw=true)



#### Forecasting with a MA (*q*)

We can once again generate a forecast using the techniques shown previously for an AR process, using the *fitted()* method from the **forecast** package:

```{r, echo=FALSE}
# Load necessary library
library(knitr)

# Time series data as a matrix for display
time_series_data <- matrix(
  c(90116.64, 80626.91, 74090.45, 38321.61, 74734.77, 101153.20, 65930.90, 106351.80,
    104138.05, 86938.99, 102868.16, 80502.02, 81466.01, 77619.15, 100984.93, 81463.10,
    61622.54, 79660.81, 88563.91, 65370.99, 104679.89, 48047.39, 73070.29, 115034.16,
    80034.03, 70052.29, 70728.85, 90437.86, 80684.44, 91533.59, 101668.18, 42273.27,
    93055.40, 68187.65, 75863.50, 40195.15, 82368.91, 90605.60, 69924.83, 54032.55,
    90866.20, 85839.41, 64932.70, 43030.64, 85575.32, 76561.14, 82047.95, 95683.35,
    66553.13, 89532.20, 85102.64, 80937.97, 93926.74, 47468.84, 75223.67, 100887.60,
    92059.32, 84459.85, 67112.16, 80917.23),
  ncol = 8, byrow = TRUE
)

# Display as a table
kable(time_series_data, col.names = paste("Value", 1:8), caption = "Time Series Values")
```

MA models exhibit strong mean reversion, so predictions quickly converge to the process mean. It makes perfect sense, given that the process is considered a function of white noise. If we 
predict beyond the model range established by its order, by definition of the process, the prediction will necessarily be its average. Consider an MA(1) model:

*y<sub>t</sub> = μ + θ<sub>1</sub> X e<sub>(t-1)</sub> + e<sub>t</sub>*

To predict a time interval in the future, our estimate for *y<sub>(t+1)</sub>* is *μ + θ<sub>1</sub> sub> + e<sub>t</sub>*. If we want to predict two time intervals in the future, 
our estimate is:

*E(y<sub>(t+2)</sub> = μ + e<sub>(t+2)</sub> + θ<sub>1</sub> X e<sub>(t+1)</sub>) = μ + 0 + θ<sub>1</sub> x 0 = μ*

With an MA(1) process we cannot present an informative prediction beyond one step forward, and for an MA(*q*) process in general we cannot present a more informative prediction beyond *q* steps 
than the average value generated through the process. By *informative* prediction, I mean one in which our most recent measurements impact our prediction.

We can check this by generating predictions with our MA model (9) that we just tuned and for which we now look for predictions ten steps ahead:

---
title: "Forecast Output Table"
output: html_document
---
<br>

##### Forecast Results

The following table shows the forecasted values along with their 80% and 95% confidence intervals.

```{r, echo=FALSE}
# Load necessary library
library(knitr)

# Create a data frame for the forecast output
forecast_data <- data.frame(
  Step = 61:70,
  `Point Forecast` = c(76320.92, 88253.99, 91767.10, 79810.12, 79361.19, 79742.03, 79606.91, 79893.84, 79988.68, 79689.81),
  `Lo 80` = c(28374.82, 40307.89, 43821.00, 26781.13, 26332.20, 26713.04, 26577.92, 26864.85, 26959.69, 26657.70),
  `Hi 80` = c(124267.0, 136200.1, 139713.2, 132839.1, 132390.2, 132771.0, 132635.9, 132922.8, 133017.7, 132721.9),
  `Lo 95` = c(2993.686, 14926.755, 18439.870, -1290.718, -1739.655, -1358.808, -1493.929, -1207.000, -1112.162, -1415.806),
  `Hi 95` = c(149648.2, 161581.2, 165094.3, 160911.0, 160462.0, 160842.9, 160707.8, 160994.7, 161089.5, 160795.4)
)

# Display the table using knitr::kable
kable(forecast_data, col.names = c("Step", "Point Forecast", "Lo 80", "Hi 80", "Lo 95", "Hi 95"), caption = "Forecast Output with Confidence Intervals")
```

When we try to predict ten steps ahead of time in the future, we predict the average for each time interval. We could have done this without a sophisticated statistical model.


## Integrated Autoregressive Moving Average Models

After analyzing the AR and MA models individually, we will examine the autoregressive integrated moving average (ARIMA) model that combines them, recognizing that the same **time series** can 
have the dynamics of the underlying AR and MA models. This alone would lead us to an ARMA model, but we will cover the ARIMA model, which takes differentiation into account, a way to remove 
trends and make a **time series** stationary.

ARIMA models continue to perform at par with the latest, especially when dealing with small data sets where the most sophisticated machine learning or deep learning models are not the best. 
However, even ARIMA models present risks of overfitting, despite their relative simplicity.

### The model

As you can see, we just fitted the same data to an AR and MA process. With that in mind, wouldn’t it be better to incorporate both behaviors in the same model?

| Chart type    | AR (*p*)      | MA (*q*)      | ARMA |
|:-------------:|:-------------:|:-------------:|:----:|
| ACF  Behavior | Slow fall                  | Sharp drop after lag = *q* | No sharp cutoff |
| PACF Behavior | Sharp drop after lag = *p* | Slow fall                  | No sharp cutoff |

This leads us to an autoregressive moving average (ARMA) model, applied to cases in which neither the terms AR nor MA are sufficient to represent the empirical dynamics. This can occur when the diagnostics for AR and MA order statistics (PACF and ACF, respectively) signal non-zero values, indicating a term of a certain order for an AR or MA term. They can be combined with a ARMA model.

##### - Wold's theorem

Wold's theorem states that every **time series** with covariance and stationary can be expressed as the sum of two **time series**, one deterministic and the other stochastic. Based on this theorem, we can also claim that a stationary process can be reasonably approximated by an ARMA model, although it is very difficult to identify the appropriate model.

More traditional statistical notation, applying negative signs to the MA process coefficients:

*y<sub>t</sub> = Ø<sub>0</sub> + $\sum_{}{}$(Ø<sub>i</sub> x r<sub>(t-1)</sub>) + e<sub>t</sub> - $\sum_{}{}$(θ<sub>i</sub> x e<sub>(t-1)</sub>)*

The stationarity of the ARMA process comes down to the stationarity of its AR component, being controlled by the same characteristic equation that controls whether an AR model is stationary. The transition from an ARMA model to an ARIMA model is simple. The difference between an ARMA model and an ARIMA model includes the term *integrated*, which refers to how many times the modeled **time series** must be differentiated to generate stationarity.

In practice, ARIMA models are more widely implemented, especially in the field of academic research and forecasting problems, than AR, MA and ARMA models.
Varieties of problems with ARIMA application:
- passengers arriving by plane;
- energy demand by types of fuel;
- daily wholesale sales;
- demand for emergency services in countries;

It is worth mentioning that the order of differentiation should not be too large. In general, the value of each parameter of an ARIMA model (*p, d, q*) should be as small as possible in order to avoid unwarranted complexity and excessive fit to the sample data. As a practical but not generalized rule, you should be suspicious of *d* values when they are above 2 and *p* and *q* values when they are above 5 or close to it. Furthermore, the value of *p* and *q* is expected to be larger and *d* to be relatively smaller. 

#### Selecting parameters

The ARIMA model is specified in terms of parameters (*p, d, q*). We will select the appropriate values of *p*, *d*, and *q* according to the data we have. ARIMA model examples from Wikipedia:

- ARIMA (0, 0, 0) is a white noise model;
- ARIMA (0, 1, 0) is a random walk and ARIMA (0, 1, 0) with a non-zero constant is a random walk with displacement;
- ARIMA (0, 1, 1) is an exponential smoothing model; and an ARIMA (0, 2, 2) is the same as the Holt linear method, which extends exponential smoothing to data with a trend so that it can be used to predict data with an underlying trend;

We will choose the order of our model based on a combination of domain knowledge, various fit assessment metrics (such as AIC), and general knowledge of how PACF and ACF should figure in a given underlying process. Afterwards, we will demonstrate the adjustment of an ARIMA model using a manual iterative process based on PACF and ACF and also an automated parameter selection tool using the *auto.arima()* function from the **forecast** package.

#### - Adjusting a model manually

There are heuristics for choosing the parameters of an ARIMA model, in which *parsimony is fundamental*. A popular and long-advocated method is the Box-Jenkins method, an iterative process with steps:

1. Use your data, visualizations, and underlying knowledge to select a model class appropriate to your data;
2. Estimate the parameters according to your training data;
3. Evaluate your model's performance based on your training data and adjust model parameters to address gaps that you identify in your performance diagnostics;

We will generate our data from an ARMA process:

![Time Series ARMA model](https://github.com/DellaCortef/time-series-pocket-reference/blob/main/statistical_models_for_time_series/images/arima-time-series-simulation.R.png?raw=true)

![ACF process of ARMA model](https://github.com/DellaCortef/time-series-pocket-reference/blob/main/statistical_models_for_time_series/images/arima-acf-arma-model.R.png?raw=true)

![PACF process of ARMA model](https://github.com/DellaCortef/time-series-pocket-reference/blob/main/statistical_models_for_time_series/images/arima-pacf-arma-model.R.png?raw=true)


We can see that neither the ACF function nor the PACF have a sharp cutoff, suggesting that we have an ARMA process. We will start by fitting a relatively simple ARIMA (1, 0, 1) model, as we see no need for differentiation and no evidence:

![ARMA () ACF and PACF model](https://github.com/DellaCortef/time-series-pocket-reference/blob/main/statistical_models_for_time_series/images/arma-acf-pacf-model.R.png?raw=true)

The residuals show huge PACF values, suggesting that we do not fully represent the autoregressive behavior. Therefore, we create the model by adding a higher-order AR component, test an ARIMA (2, 0, 1) model in the following code, and then plot the ACF and PACF of the residuals from this more complex model:

![AR2MA1 model](https://github.com/DellaCortef/time-series-pocket-reference/blob/main/statistical_models_for_time_series/images/arma-testing-model.R.png?raw=true)

![ACF and PACF model](https://github.com/DellaCortef/time-series-pocket-reference/blob/main/statistical_models_for_time_series/images/ar2ma1-acf-pacf-model.R.png?raw=true)

The residuals no longer show huge values ​​for the respective ACF and pACF. Given our desire for a parsimonious model and the risks of overfitting an ARIMA model, a wise analyst would stop right here, as we do not have any additional behavior in the residuals that would need to be adjusted through an autoregressive, moving average, or differencing component. . We can bring more complexity by adjusting MA(2) to our model:

![AR2MA2 model](https://github.com/DellaCortef/time-series-pocket-reference/blob/main/statistical_models_for_time_series/images/arma-ar2ma2-model.R.png?raw=true)

![ACF and PACF model](https://github.com/DellaCortef/time-series-pocket-reference/blob/main/statistical_models_for_time_series/images/arma-ar2ma2-acf-pacf.R.png?raw=true)

There is a quick way to compare models, in which we will check how the predictions of a fitted model correlate with the actual values:

| Model    | COR       | 
|:--------:|:---------:|
| AR1MA1   | 0.7797449 |
| AR2MA1   | 0.7800458 |
| AR2MA2   | 0.7802289 |

We observed little substantial improvement in the transition from an ARIMA (1, 0, 1) model to an ARIMA (2, 0, 1) model. On the other hand, as we added more complexity, we did not see any substantial improvement in correlation. This validates that previously the ARIMA (2, 0, 1) model represented the model behavior well, without the need to add more AR or MA components to improve the fit.

With this, we can observe how well the adjustment performed, comparing the original adjusted coefficients with the adjusted coefficients:

| Model     | COR       | 
|:--------: |:---------:|
| ar1       |  0.58357893 |
| ar2       | -0.06148804 |
| ma1       |  0.48787662 |
| intercept | -0.04732098 |

Tuning an ARIMA model manually is more complex than just the steps taken here. We must not forget that there are valid criticisms about "manual" tuning of an ARIMA process. Manually adjusting a model can be a process that leaves something to be desired in terms of specification, as the analyst's representation ends up under a lot of pressure, takes time and depends on the final result.

##### - Using automated model tuning

Nowadays, we can forego a manual interactive adjustment process and use, depending on the case, an automated model. Model selection will depend on several information loss criteria, such as the AIC seen briefly through the *auto.arima()* function in the **forecast** package:<br><br>

---
title: "ARIMA Model Output"
output: html_document
---

---
title: "ARIMA Model Output"
output: html_document
---

##### ARIMA Model Results

##### - Model Information

- **Series**: demand_banking
- **Model**: ARIMA(0,0,3) with non-zero mean

##### - Coefficients Table

```{r, echo=FALSE}
# Load the knitr library for table formatting
library(knitr)

# Create a data frame for the coefficients and standard errors
coefficients <- data.frame(
  Term = c("ma1", "ma2", "ma3", "mean"),
  Estimate = c(-0.0645, -0.1144, -0.4796, 79914.783),
  `Std. Error` = c(0.1327, 0.1150, 0.1915, 1897.407)
)

# Display the table with knitr::kable
kable(coefficients, col.names = c("Term", "Estimate", "Std. Error"), caption = "ARIMA Model Coefficients")
```

We use a one-liner with prior knowledge from our previous exploration to define the inputs. That is, we specified the maximum order of the AR and MA process that we were prepared to accept, and in fact model selection chose a more parsimonious model than we specified, without any AR terms. However, this model fits well and we should not resort to data snooping unless we have a justified reason to do so. Note that, according to the AIC criteria, our previously chosen manually MA model performs slightly better than the automatically chosen model, but when we look at the graphs, the difference does not seem significant.

We can analyze how *auto.arima()* would be executed in the model we generated previously:<br><br>

---
title: "ARIMA Model Output"
output: html_document
---

##### ARIMA Model Results

##### - Model Information

- **Series**: y
- **Model**: ARIMA(3,0,1) with zero mean

##### - Coefficients Table

```{r, echo=FALSE}
# Load knitr for table formatting
library(knitr)

# Create a data frame for the coefficients and standard errors
coefficients <- data.frame(
  Term = c("ar1", "ar2", "ar3", "ma1"),
  Estimate = c(0.6242, -0.3537, -0.0526, -0.5586),
  `Std. Error` = c(0.0847, 0.0384, 0.0473, 0.0798)
)

# Display the table with knitr::kable
kable(coefficients, col.names = c("Term", "Estimate", "Std. Error"), caption = "ARIMA Model Coefficients")
```

We didn't even use the optional parameters to suggest to *auto.arima()* where it should start its model search, and yet, it converged on the same solution with different methodologies.

We perform our analysis by observing the ACF and PACF of residuals from simpler models in order to develop more complex models, while *auto.arima()* is largely driven by grid search that minimizes AIC. Obviously, since we generated the original data from an ARIMA process, this represents a simpler case than many real-world data. In this case, our manual adjustments and automatic model selection do not always reach the same conclusion.

<br><br>

## Vector Autoregression

In practice, we sometimes have a variety of parallel **time series** presented in relation to each other. We've already looked at how to clean and align this data, and now we can learn how to make the most of it. It is possible to do this by generating an AR model (*p*) for the case of multiple variables. The fun of this model is that it predicts the fact that variables influence each other and are in turn influenced - that is, there is no privileged y while everything else is designated as x. On the contrary, the adjustment is symmetric in all variables. Note that differentiation can be applied as in other previous models, if the **time series** is not stationary.

Since each **time series** predicts all others as well as itself, we will have one option per variable. Let's say we have three **time series**: we will represent their value at time *t* as *y<sub>1, t</sub>* and *y<sub>2, t</sub>* and * y<sub>3, t</sub>*. Thus, we can write the geometric autoregression equations (VAR) of order 2 (factoring 2 time lags) as:

*y<sub>1, t</sub> = Ø<sub>01</sub> + Ø<sub>11,1</sub> x y<sub>(1, t-1)</sub> + Ø<sub>12,1</sub> x y<sub>(2, t-1)</sub> + Ø<sub>13,1</sub> x y<sub>(3, t-1)</sub> + Ø<sub>11,2</sub> x y<sub>(1, t-2)</sub> + Ø<sub>12,1</sub> x y<sub>(2, t-1)</sub> + Ø<sub>13,1</sub> x y<sub>(3, t-1)</sub>*

*y<sub>2, t</sub> = Ø<sub>02</sub> + Ø<sub>21,1</sub> x y<sub>(1, t-1)</sub> + Ø<sub>22,1</sub> x y<sub>(2, t-1)</sub> + Ø<sub>23,1</sub> x y<sub>(3, t-1)</sub> + Ø<sub>21,2</sub> x y<sub>(1, t-2)</sub> + Ø<sub>22,1</sub> x y<sub>(2, t-1)</sub> + Ø<sub>23,1</sub> x y<sub>(3, t-1)</sub>*

*y<sub>2, t</sub> = Ø<sub>03</sub> + Ø<sub>31,1</sub> x y<sub>(1, t-1)</sub> + Ø<sub>32,1</sub> x y<sub>(2, t-1)</sub> + Ø<sub>33,1</sub> x y<sub>(3, t-1)</sub> + Ø<sub>31,2</sub> x y<sub>(1, t-2)</sub> + Ø<sub>32,1</sub> x y<sub>(2, t-1)</sub> + Ø<sub>33,1</sub> x y<sub>(3, t-1)</sub>*

##### - Multiplicação de Matriz

Express the relationships shown in the previous three equations via matrix notation. It is possible to write a VAR in a very similar way to an AR. In matrix form, the three equations can be expressed as:

*y = Ø<sub>0</sub> + Ø<sub>1</sub> x y<sub>(t-1)</sub> + Ø<sub>2</sub> x y<sub>( t-2)</sub>*

where *y* and *Ø* are 3 x 1 matrices and the other matrices *Ø* are 3 x 3

Even in a simple case, we notice that the number of parameters in the model grows dramatically. For example, if we have lag *p* and variables *N*, we can see that the predictor equation for each variable is 1 + *p* x *N*. As we will have *N* values to predict, this translates into the total variables *N* + *p* x *$N^2$*, meaning that the number of variables grows in proportion O($N^2$) to the number of **time series** studied. Therefore, we should not use **time series** lightly simply because we have data. We should reserve this method for when we are in a relationship.

VAR models are most frequently used in econometrics. They receive criticism because they have no structure other than the hypothesis that all values ​​influence each other. It is precisely for this reason that the quality of model fit can be difficult to assess. However, VAR models still have a use - for example, to test whether one variable causes another variable. They are also useful for situations where it is necessary to predict several variables that we do not have domain knowledge in order to assert any specific type of relationship. They can also sometimes serve to determine how much variance in a prediction of a value is attributable to its underlying "causes."

We will analyze the underlying UCI demand information and consider using a second column to predict bank orders instead of your own data (note that we will also predict this column due to the symmetric way in which the variables are handled). We will use orders from the traffic control department. Apparently, they should be different, so they can provide an independent source of information in relation to previous orders from the tax sector itself. We can also imagine that each column provides underlying information about economic progress and whether demand will increase or decrease in the future

To determine which parameters to use, we will use the **vars** package, which has the *VARselect()* method:


