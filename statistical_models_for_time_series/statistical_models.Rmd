---
title: "Statistical Models for Time Series"
author: "Della"
date: "2024-11-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Statistical Models for Time Series

We will map some linear statistical models to **time series**. These models are related to linear regression, but represent correlations between data points in the same **time series**, 
unlike standard methods applied to cross-sectional data, where each data point is assumed to be independent of the others in the sample. We will analyze the following models:
  - autoregressive models (AR), moving average models (MA) and models 
  autoregressive integrated moving average (ARIMA);
  - vector autoregression (VAR);
  - hierarchical models;
  
Traditionally, these models have been the driving force in forecasting **time series** and continue to be used in a wide range of situations, from academic research to modeling 
in various fields of acting


## Why Not Use Linear Regression
As a data analyst, chances are you are already familiar with *linear regressions*. 
If not, a *linear regression* assumes you has *independent and identically distributed data (iid)*. According to we studied previously, this does not occur with **series data temporal**. 
In them, nearby points in time are usually strongly correlated with each other. In reality, when there are no correlations temporal, **time series** data are hardly useful for 
traditional tasks, such as predicting the future or understanding dynamics temporal.

It is not uncommon for tutorials and books on **time series** to teach us undue impression that *linear regression* is not useful for **series temporal**. What makes students believe 
that *linear regressions* simple are not enough. But that's not how it works. The *regression ordinary least squares linear* can be applied to data **time series**, 
provided that the following conditions are met:
*Assumptions regarding the behavior of the **time series**:*
- the **time series** has a linear response to its predictors;
- no input variable is constant over time or perfectly correlated with another input variable. Thus, the traditional requirement of *linear regression* of independent variables if 
amplifies to consider the temporal dimension of the data.
*Assumptions regarding the error:*
- for each point in time, the expected value of the error given all 
explanatory variables for all time periods (step forward 
[forward] and step backward [backward]), is 0;
- error in any given time period does not correlate with entries in no time period in the past or future. Therefore, a graph of the autocorrelation function of the errors 
will not indicate no default;
- the error variance is independent of time;

If these assumptions are valid, *least squares regression ordinary* is an unbiased estimator of the coefficients given to the inputs, even for **time series** data. In this case, 
the variances of the sampled estimates have the same mathematical form for the *regression linear* pattern. Therefore, if your data meets the assumptions listed, we can apply a 
*linear regression* that will undoubtedly help to offer clear and simple instructions on the behavior of the **time series**. The data requirements just described are similar to those 
in *standard linear regression* applied to cross-sectional data.Consequences of using *linear regression* outside the standards:
- coefficients will not minimize the model error;
- *p-values* to determine if your coefficients are non-zero will be incorrect as they are based on assumptions that are not answered. This means that your assessments of the 
significance of the coefficient may be wrong. Linear regressions serve to enable simplification and transparency when appropriate, but an incorrect model is not transparent at all.

The importance of following the assumptions of a model depends largely on the domain in question. Sometimes a model is applied with full knowledge that the basic assumptions are not 
met, as the consequences justify the benefits. For example, in high frequency trading, linear models are quite popular for many reasons despite no one believes that the data 
strictly follows all default assumptions.


## Statistical Methods Developed for Time Series

We will analyze statistical methods developed exclusively for data of **time series**. First, we will study the methods developed for **time series** *univariate* data, starting with 
a simple autoregressive model that states that the future values of a **time series** are a function of their past values. Afterwards, we will move on to few for more complex models, 
and we will conclude our study with a analysis on vector autoregression for **time series** *multivariate* and some specialized **time series** methods, such as *GARCH* models and 
hierarchical modeling.

### Autoregressive Models

The autoregressive (AR) model is based on the intuition that the past predicts the future. In this way, they presuppose a **series process temporal** in which the value at a point in 
time *t* is a function of the series values at previous points in time.

And so that we have an idea of how statisticians use these methods and its properties, our analysis will be thorough. Therefore, we will start with a comprehensive theoretical overview. If you are not interested in technicality of how the properties of statistical models are calculated in **time series**, just take a look at this section.<br><br>

#### Using algebra to understand constraints in AR processes

Autoregression resembles what many people would use as their first attempt to adjust a **time series**, especially if you do not have no information other than the **time series** 
itself. It's exactly what its name says: a regression on past values to predict values futures. The simplest AR model is represented like this:

y<sub>(1)</sub> = b<sub>(0)</sub> + b<sub>(1)</sub> * y<sub>(t-1)</sub> + e<sub>(t)</sub>
 
- The value of the series at time *t* is a function of a constant *b<sub>(0)</sub>*, its value in the previous time interval multiplied by another constant 
*b<sub>(1)</sub>* * *y<sub>(t-1)</sub>* and an error term that also varies with time *e<sub>(t)</sub>*. It is assumed that this error term has a variance constant and a mean of 0. 
We represent an autoregressive term as a AR(1) model that considers the past only in the immediately previous moment, as it includes a lag lookback. By the way, the AR(1) model is 
identical in form to a simple *linear regression* model with just one explanatory variable. That is, it maps: y<sub>(1)</sub> = b<sub>(0)</sub> + b<sub>(1)</sub> * x + e
- We can calculate the expected value of y<sub>(1)</sub> and its variance given y<sub>(t-1)</sub> if we know the value b<sub>(0)</sub> and b<sub>(1)</sub>:

*E(y<sub>(t)</sub>|y<sub>(t-1)</sub>) = b<sub>(0)</sub> + b<sub>(1)</sub> x y<sub>(t-1)</sub> + e<sub>(t)</sub>*

*Var(y<sub>(t)</sub>y<sub>(t-1)</sub>) = Var(e<sub>(t)</sub>) = Var(e)*

- The generalization of the previous notation allows the present value of an AR process to depend on the most recent values *p*, generating a process AR(*p*). Now we switch to a more 
traditional notation, which uses Ø to represent autoregression coefficients:
*y<sub>(t)</sub> = Ø + Ø<sub>(1)</sub> x y<sub>(t-1)</sub> + Ø<sub>(2)</sub> x y<sub>(t-2)</sub> + ... + + Ø<sub>(p)</sub> x y<sub>(t-p)</sub> + e<sub>(t)</sub>*

As we analyzed previously, stationarity is a fundamental concept in the analysis of **time series**, as it is the requirement of many models, including AR models. We can determine the 
conditions for an AR model to be stationary from the definition of stationarity:

*y<sub>(t)</sub> = Ø<sub>(0)</sub> + Ø<sub>(1)</sub> x y<sub>(t-1)</sub> + e<sub>(t)</sub>*

We assume that the process is stationary. So we work “one step backward” to see how this impacts the coefficients. First, based on the assumption of stationarity, we know that the 
expected value of the process must be the same at all times. We can rewrite y<sub>(t)</sub>:

*E(y<sub>(t)</sub>) = μ = E(y<sub>(t-1)</sub>)*

By definition, *e<sub>(t)</sub>* has an expected value of 0. Furthermore, the phis (φ) are constants, so their expected values are their constant values.

*E(y<sub>(t)</sub>) = E(Ø<sub>(0)</sub> + Ø<sub>(1)</sub> x y<sub>(t-1)</sub> + e<sub>(t)</sub>)*  
*E(y<sub>(t)</sub>) = μ*  
*Ø<sub>(0)</sub> = Ø<sub>(1)</sub> x μ + 0*  <br><br>
Simplified to:  
*μ = Ø<sub>(0)</sub> = Ø<sub>(1)</sub> μ*

Thus, we find a relationship between the process mean and the underlying AR(1) coefficients. <br><br>
We can follow a similar approach to see how constant variance and covariance impose conditions on the Ø coefficients. Replacing the value of Ø<sub>(0)</sub>, deriving the 
previous equation:

*Ø<sub>0</sub> = μ x (1 - Ø<sub>1</sub>)*<br>
*y<sub>t</sub> = Ø<sub>0</sub> + Ø<sub>1</sub> x y<sub>(t-1)</sub> + e<sub>t</sub>*<br>
*y<sub>t</sub> = (μ = μ x Ø<sub>1</sub>) + Ø<sub>1</sub> x y<sub>(t-1)</sub> + e<sub>t</sub>*<br>
*y<sub>t</sub> - μ = Ø<sub>1</sub>(y<sub>(t-1)</sub> - μ)+ e<sub>t</sub>*<br>

Since this **time series** is stationary, we know that the calculation at time *t*-1 must be the same as that at time *t*:

*y<sub>(t-1)</sub> - μ = Ø<sub>1</sub>(y<sub>(t-2)</sub> - μ) + e<sub>(t-1)</sub>*

Then we can replace:

*y<sub>t</sub> - μ = Ø<sub>1</sub>(Ø<sub>1</sub>(y<sub>(t-2)</sub> - μ) + e<sub>(t-1)</sub>) + e<sub>t</sub>*

Rearranging:

*y<sub>t</sub> - μ = e<sub>t</sub> + Ø<sub>1</sub>(e<sub>(t-1)</sub> + Ø<sub>1</sub>(y<sub>(t-2)</sub> - μ))*

Instead of working on *y<sub>(t-1)</sub>*, we will work on *y<sub>(t-2)</sub>*:

*y<sub>t</sub> - μ = e<sub>t</sub> + Ø<sub>1</sub>(e<sub>(t-1)</sub> + Ø<sub>1</sub>(e<sub>(t-2)</sub> + Ø<sub>1</sub>(y<sub>(t-3)</sub> - μ))) = e<sub>t</sub> + Ø x e<sub>(t-1)</sub> + $Ø^2$ x e<sub>(t-2)</sub> + $Ø^3$ x e<sub>(t-3)</sub>*<br><br>


With this, we can conclude more generally that  **y<sub>t</sub> - μ = $\sum_{i=1}^{\infty}$ Ø<sub>1</sub>$^i$ x e<sub>(t-1)</sub>**<br><br>


This result can then be used to calculate the expected *E[(y<sub>t</sub> - μ) x e<sub>(t+1)</sub>] = 0*, given that the values of *e<sub>t+1</sub>* at different values of *t* they are 
independent. From this, we can conclude that the covariance of *y<sub>(t-1)</sub>* e *e<sub>t</sub>* is 0, as it should be. We can use similar logic to calculate the variance of 
*y<sub>t</sub>* squaring this equation:

*y<sub>t</sub> - μ = Ø<sub>1</sub>(y<sub>(t-1)</sub> - μ + e<sub>t</sub>*

*var(y<sub>t</sub>) = Ø<sub>1</sub>$^2$ var(y<sub>(t-1)</sub>) + var(e<sub>t</sub>)*

Since the variance quantities on each side of the equation must be equal due to stationarity, this means:

*var(y<sub>t</sub>) = $\frac{\text{var}(e_t)}{1 - \phi_1^2}$*

Considering that the variance must be greater than or equal to 0 by definition, we can see that *Ø<sub>1</sub>$^2$* must be less than 1 to guarantee a positive value on the right side 
of the previous equation. that is, for a stationary process, it must have -1 < *Ø<sub>1</sub>* < 1. This is a necessary and sufficient condition for this type of weak stationarity.<br><br>


#### Choosing parameters for an AR(p) model

To evaluate the suitability of an AR model on the data, we will start by plotting the process and its *partial autocorrelation function (PACF)*. The PACF of an AR process must be 
reduced to zero beyond the order *p* of an AR process (*p*), providing concrete and visual indications of the order of an AR process seen empirically in the data.

In contrast, an AR process will not have an informative autocorrelation function (ACF), although it will have the characteristic shape of an ACF: exponential tapering with increasing 
time offset.

Let's analyze this with some real data. We will use demand forecast data published by the **UCI Machine Learning Repository** (*https://perma.cc/B7EQ-DNLU*). First, we will 
plot the data in chronological order:

![Alt text](https://github.com/DellaCortef/time-series-pocket-reference/blob/main/statistical_models_for_time_series/images/pacf-banking-orders-2-demand.R.png?raw=true/pacf-banking-orders-2-demand.R.png)
We can observe that the PACF value exceeds the significance threshold of 5% in lag 3. This is compatible with the results of the *ar()* function available in the **stats** package of 
**R**. The *ar()* function automatically chooses the order of an autoregressive model if none is specified:

---
title: "AR Model Output"
output: html_document
---

##### AR Model Results

##### - Call
`ar(x = demand_banking, method = "mle")`

##### - Coefficients
```{r, echo=FALSE}
# Define the coefficients in a data frame
coefficients <- data.frame(
  Order = 1:3,
  Coefficient = c(-0.1360, -0.2014, -0.3175)
)

# Display the table with knitr::kable
knitr::kable(coefficients, col.names = c("Order", "Coefficient"))
```

If we look at the documentation for the *ar()* function (*https://perma.cc/8H8Z-CX9R*), we can see that the selected order is determined (with default parameters that we do not change) 
it shows that the visual selection we made examining PACF is compatible with the selection that would be made minimizing an information criterion. Although there are two different ways 
of choosing the model order, here they are compatible.

Note that the *ar()* function also provided us with the coefficients for the model. However, we can restrict the coefficients. For example, when analyzing the PACF, we may ask ourselves 
whether we want to include a coefficient for the lag -1 term or whether we should assign that term a mandatory coefficient of 0, given that its PACF value is well below the threshold 
used for significance. In this case, we can use the *arima()* function, also from the **stats** package.

Next, we will demonstrate how to call the function to adjust an AR (3) by setting the ordering parameter to c(3, 0, 0), where 3 refers to the order of the AR component:















