---
title: "Statistical Models for Time Series"
author: "Della"
date: "2024-11-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Statistical Models for Time Series

We will map some linear statistical models to **time series**. These models are related to linear regression, but represent correlations between data points in the same **time series**, 
unlike standard methods applied to cross-sectional data, where each data point is assumed to be independent of the others in the sample. We will analyze the following models:
  - autoregressive models (AR), moving average models (MA) and models 
  autoregressive integrated moving average (ARIMA);
  - vector autoregression (VAR);
  - hierarchical models;
  
Traditionally, these models have been the driving force in forecasting **time series** and continue to be used in a wide range of situations, from academic research to modeling 
in various fields of acting


## Why Not Use Linear Regression
As a data analyst, chances are you are already familiar with *linear regressions*. 
If not, a *linear regression* assumes you has *independent and identically distributed data (iid)*. According to we studied previously, this does not occur with **series data temporal**. 
In them, nearby points in time are usually strongly correlated with each other. In reality, when there are no correlations temporal, **time series** data are hardly useful for 
traditional tasks, such as predicting the future or understanding dynamics temporal.

It is not uncommon for tutorials and books on **time series** to teach us undue impression that *linear regression* is not useful for **series temporal**. What makes students believe 
that *linear regressions* simple are not enough. But that's not how it works. The *regression ordinary least squares linear* can be applied to data **time series**, 
provided that the following conditions are met:
*Assumptions regarding the behavior of the **time series**:*
- the **time series** has a linear response to its predictors;
- no input variable is constant over time or perfectly correlated with another input variable. Thus, the traditional requirement of *linear regression* of independent variables if 
amplifies to consider the temporal dimension of the data.
*Assumptions regarding the error:*
- for each point in time, the expected value of the error given all 
explanatory variables for all time periods (step forward 
[forward] and step backward [backward]), is 0;
- error in any given time period does not correlate with entries in no time period in the past or future. Therefore, a graph of the autocorrelation function of the errors 
will not indicate no default;
- the error variance is independent of time;

If these assumptions are valid, *least squares regression ordinary* is an unbiased estimator of the coefficients given to the inputs, even for **time series** data. In this case, 
the variances of the sampled estimates have the same mathematical form for the *regression linear* pattern. Therefore, if your data meets the assumptions listed, we can apply a 
*linear regression* that will undoubtedly help to offer clear and simple instructions on the behavior of the **time series**. The data requirements just described are similar to those 
in *standard linear regression* applied to cross-sectional data.Consequences of using *linear regression* outside the standards:
- coefficients will not minimize the model error;
- *p-values* to determine if your coefficients are non-zero will be incorrect as they are based on assumptions that are not answered. This means that your assessments of the 
significance of the coefficient may be wrong. Linear regressions serve to enable simplification and transparency when appropriate, but an incorrect model is not transparent at all.

The importance of following the assumptions of a model depends largely on the domain in question. Sometimes a model is applied with full knowledge that the basic assumptions are not 
met, as the consequences justify the benefits. For example, in high frequency trading, linear models are quite popular for many reasons despite no one believes that the data 
strictly follows all default assumptions.


## Statistical Methods Developed for Time Series

We will analyze statistical methods developed exclusively for data of **time series**. First, we will study the methods developed for **time series** *univariate* data, starting with 
a simple autoregressive model that states that the future values of a **time series** are a function of their past values. Afterwards, we will move on to few for more complex models, 
and we will conclude our study with a analysis on vector autoregression for **time series** *multivariate* and some specialized **time series** methods, such as *GARCH* models and 
hierarchical modeling.

### Autoregressive Models

The autoregressive (AR) model is based on the intuition that the past predicts the future. In this way, they presuppose a **series process temporal** in which the value at a point in time *t* is a function of the series values at previous points in time.

And so that we have an idea of how statisticians use these methods and its properties, our analysis will be thorough. Therefore, we will start with a comprehensive theoretical overview. If you are not interested in technicality of how the properties of statistical models are calculated in **time series**, just take a look at this section.

#### Using algebra to understand constraints in AR processes

Autoregression resembles what many people would use as their first attempt to adjust a **time series**, especially if you do not have no information other than the **time series** itself. It's exactly what its name says: a regression on past values to predict values futures. The simplest AR model is represented like this:

y<sub>(1)</sub> = b<sub>(0)</sub> + b<sub>(1)</sub> * y<sub>(t-1)</sub> + e<sub>(t)</sub>
 
- The value of the series at time *t* is a function of a constant *b<sub>(0)</sub>*, its value in the previous time interval multiplied by another constant *b<sub>(1)</sub>* * *y<sub>(t-1)</sub>* and an error term that also varies with time *e<sub>(t)</sub>*. It is assumed that this error term has a variance constant and a mean of 0. We represent an autoregressive term as a AR(1) model that considers the past only in the immediately previous moment, as it includes a lag lookback. By the way, the AR(1) model is identical in form to a simple *linear regression* model with just one explanatory variable. That is, it maps: y<sub>(1)</sub> = b<sub>(0)</sub> + b<sub>(1)</sub> * x + e
- We can calculate the expected value of y<sub>(1)</sub> and its variance given y<sub>(t-1)</sub> if we know the value b<sub>(0)</sub> and b<sub>(1)</sub>:

E(y<sub>(t)</sub>|y<sub>(t-1)</sub>) = b<sub>(0)</sub> + b<sub>(1)</sub> * y<sub>(t-1)</sub> + e<sub>(t)</sub>
Var(y<sub>(t)</sub>y<sub>(t-1)</sub>) = Var(e<sub>(t)</sub>) = Var(e)
- The generalization of the previous notation allows the present value of an AR process to depend on the most recent values *p*, generating a process AR(*p*). Now we switch to a more traditional notation, which uses Ã˜ to represent autoregression coefficients: