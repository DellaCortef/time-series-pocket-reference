---
title: "Statistical Models for Time Series"
author: "Della"
date: "2024-11-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Statistical Models for Time Series

We will map some linear statistical models to **time series**. These models are related to linear regression, but represent correlations between data points in the same **time series**, 
unlike standard methods applied to cross-sectional data, where each data point is assumed to be independent of the others in the sample. We will analyze the following models:
  - autoregressive models (AR), moving average models (MA) and models 
  autoregressive integrated moving average (ARIMA);
  - vector autoregression (VAR);
  - hierarchical models;
  
Traditionally, these models have been the driving force in forecasting **time series** and continue to be used in a wide range of situations, from academic research to modeling 
in various fields of acting


## Why Not Use Linear Regression
As a data analyst, chances are you are already familiar with *linear regressions*. 
If not, a *linear regression* assumes you has *independent and identically distributed data (iid)*. According to we studied previously, this does not occur with **series data temporal**. 
In them, nearby points in time are usually strongly correlated with each other. In reality, when there are no correlations temporal, **time series** data are hardly useful for 
traditional tasks, such as predicting the future or understanding dynamics temporal.

It is not uncommon for tutorials and books on **time series** to teach us undue impression that *linear regression* is not useful for **series temporal**. What makes students believe that *linear regressions* simple are not enough. But that's not how it works. The *regression ordinary least squares linear* can be applied to data **time series**, provided that the following conditions are met:
