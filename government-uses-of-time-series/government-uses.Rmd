---
title: "Government Uses"
author: "Della"
date: "2025-02-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

For several reasons, **time series** analysis is pertinent and invaluable for government uses. First, governments, large and small, are custodians of some of the world's most important time series data, including the US jobs report, ocean temperature data, and local crime statistics. Second, governments, by definition, provide some of the most essential services we all depend on. Therefore, they need to be reasonably skillful forecasters of demand if they do not want to overspend or understaff these services. Thus, all aspects of **time series** are relevant to government objectives: storage, cleaning, exploration and forecasting.

As we mentioned earlier, when analyzing "found" **time series**, a relatively high percentage of all government data can look a lot like **time series** data if we restructure it a bit. In general, most government datasets are the result of continuous data collection, not a single chunk of time. However, government datasets can be scary for several reasons:

- inconsistent data records (due to organizational constraints or political forces changing over time);
- shady or confusing data practices;
- gigantic set of data with relatively low information content.

However, it can be very beneficial to analyze government datasets for both intellectual interest and many practical purposes. Now, we will explore a government dataset consisting of all complaints filed in New York City from 2010 to present (https://perma.cc/BXF6-BZ4X) through a city hotline that can be accessed by dialing 311. We will cover the following topics:

- interesting sources of government data, including the one we will analyze;
- deal with gigantic plain text data files;
- online/continuous statistical analysis of large datasets and other options to analyze data without keeping it in memory.

## Obtaining Government Data

Government datasets that fall into the “found data” category can be a nightmare from a data consistency standpoint. These datasets, although timestamped, are generally made available for open source initiatives and not for a specific **time series** purpose. Often, there is little or no information available about data timestamp conventions or other recording conventions. It can be difficult to confirm that underlying recordkeeping practices were consistent.

But if you are curious or want to be the first to identify interesting temporal characteristics in human behaviors related to government activities, you are lucky to live in the open source government era. In recent years, regardless of government hierarchies, many governments have strived hard to make their **time series** data transparent to the public. Below, we'll look at just a few examples of where we can get open government data with a **time series** component:

- monthly hospital data (https://perma.cc/4TR3-84WA) from the UK National Health Service. This dataset is very time series aware: it includes a tab called "MAR timeseries" and describes the recording conventions and how they have evolved over time;
- Jamaica's open data portal also includes an assessment and recognition of **time series** data, such as its timestamped dataset on Chikungunya cases (https://perma.cc/4RCP-VMY6) from 2014 and the associated data report (https://perma.cc/QPR6-WNMJ), which includes an animation (i.e. a **time series** visualization) and an epidemiological curve;
- Singapore's open data portal (https://perma.cc/N9W4-ZDM8) presents extensive data sets and discloses the **time series** nature of some of this data, including two **time series** charts.

## Exploring Big Time Series Data

When the data is large enough, we will not be able to put it into memory. How big the data needs to be before reaching this limit will depend on the hardware we are using. Sooner or later, we'll need to understand how to iterate through your data, one manageable piece at a time. Those familiar with deep learning have probably already done this, especially if they have worked in image processing. Deep learning frameworks have made available Python iterators that work their way through a dataset, and these datasets are stored in specific directories, each with many files.

At the time of testing, the 311 dataset was over 3 gigabytes in CSV format. There was no way to open this on the machine, so the first idea was to use the standard Unix system options, such as *head*. Unfortunately, what was displayed was already so large as to be impossible to manage in a Unix command-line interface:

```Linux
head 311.csv
```

Although the content is heavy, this preview was enough to show that there were multiple timestamps as well as other interesting and orderly information like geographic coordinates. Of course, the data is vast, so we will need to be able to manipulate this information to obtain the columns we want.

Even if you are new to Linux, you can easily learn simple command-line tools that can provide you with useful information. We can get a row count from the CSV file to get a sense of the scale we are analyzing, that is, how many data points we have:

```Linux
wc -l 311.csv
```

We can see that NYC has received 1,202,182 complaints through its 311 hotline. We use data from 2024 to today.

Armed with this knowledge, we will use R's *data.table*, as we know that its *fread()* function allows partial reading of files (https://perma.cc/ZHN9-5HD3). Note the *nrows()* and *skip()* parameter part. We also know that *data.table* is extremely efficient when dealing with large data sets. We can use them to obtain initial information, like this:

```R
# Install packages
install.packages("data.table")

# Load packages
library(data.table)

# Setting the dataset path
setwd('/Users/dellacorte/py-projects/data-science/time-series-pocket-reference/datasets/')

df = fread("311.csv", skip = 0, nrows = 10)
colnames(df)
```

Just read ten lines and we can see the column names. Of all the advantages I've listed of NoSQL approaches to **time series** data, it can be nice, in a large dataset, to know the column names from the beginning. Of course, there are alternative solutions with NoSQL data, but most require a little effort on the part of the user, as things don't flow automatically. Several columns suggest useful information:

  "Created Date"
  
  "Closed Date"
  
  "Due Date"
  
  "Resolution Action Updated Date"
  
These columns will likely be of type character before we convert them, but once we do a conversion to a POSIXct type, we can examine what the time interval between dates looks like:

```R
df[, created_date := as.POSIXct(created_date, format = "%m/%d/%Y %I:%M:%S %p")]
```

In the format string, we need to use %I for the time, as it is only expressed in the format 01-12, and %p because timestamps are dammed as AM/PM. To get a sense of how spaced these dates tend to be, especially when it comes to when a claim is opened or closed, we'll load more rows and look at the distribution that I'll call the *claim period* (i.e., the interval between opening and closing a claim):

```R
> summary(df$duration_days)
```

---
title: "Summary Statistics Table"
output: html_document
---

```{r, echo=FALSE, message=FALSE}
# Load required libraries
library(knitr)
library(kableExtra)

# Create summary statistics table
summary_stats <- data.frame(
  Statistic = c("Min.", "1st Qu.", "Median", "Mean", "3rd Qu.", "Max.", "NA's"),
  Value = c(-66.00, 0.06, 0.38, 3.34, 2.20, 114.62, 99777)
)

# Render the table in HTML format
kable(summary_stats, format = "html", caption = "Summary Statistics of Duration in Days") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = FALSE)
```

As we can see, it is a wide distribution. More impressive than the long years of waiting for some complaints to be closed is the fact that some of them have negative, even extremely negative, results between their creation and the closing date. If the negative time was around -365 days, we could imagine a data entry problem, but this seems less likely with numbers like -66 days. It's the kind of problem we need to look at. We can identify another one, when we get a creation date range:

```R
> range(df$created_date)
[1] "2024-10-01 16:09:24 -03" "2025-01-26 03:33:15 -03"
```

Given the size of this CSV file (three months or so) and the fact that it must be constantly updated, it is a wonder that the first few lines are not from 2010, which should mark the first datasets. We expected the CSV to be continually appended. And even more surprising is the fact that the dates of 2024 are in the first lines. This suggests that we cannot determine the date ordering of the data in the file. We can visualize the date distribution from one line to the next by plotting a line graph on the index against the date:

![](https://github.com/DellaCortef/time-series-pocket-reference/blob/main/government-uses-of-time-series/images/cumulative-record-count-over-time.png?raw=true)

There is no way to avoid this problem. If we want to understand how behavior changes over time, we will have to deal with unordered data. But we have some options.

### Use Upsampling and aggregate Data as we iterate

One such option is to upsample the data as we iterate through it to build a condensed **time series** with aggregated statistics. We could choose a resolution and aggregation functions from the beginning of our analysis and then calculate them as we iterate. Then we could classify our results at the end of our analysis. It would be like having, say, a dictionary/list of all dates from 2024 (or earlier, 2010 for example) to the present and then simply adding the appropriate date to each line. This method would generate a list/dictionary with a relatively small number of entries and we could then sort them at the end based on date.

The advantage is that this would be relatively simple to code and would be a way to combine cleaning, exploration. data analysis in an exploratory stage. The disadvantage is that the detailed data would still be lost in the limbo of the unclassified file so that if there was a specific period of interest, we would have to search the entire unclassified file in order to find all the relevant entries.

### Classifying Data

Another option is to sort the data. It is a Herculean task, given the gigantic size of the file and the relatively disordered dates that we were able to observe. Even in the 2014 data, it doesn't appear that the dates are in order. Therefore, we have no indication that we can trust any data. In other words, we must consider this a pile of data in random order.

Sorting the entire file would require a lot of memory, but it might be worth it for two reasons. One of them is that we just classify once, and then we can save our results for any later analysis we want to do. The other is that we can have the full level of detail preserved. This way, if we identify specific time periods of interest in our analysis, we can examine the data in all its detail to understand what is happening. In concrete terms, and thinking about how we could do this, we have some options:

- Linux has a command line tool for sorting;
- most databases can classify data, so we can transfer it to a database and let the bank take care of this process;
- we could create our own sorting algorithm and implement it. It is necessary to formulate something that does not consume a huge amount of memory. The odds of our efforts matching what is available in pre-packaged rating options are slim.

We chose to use a Linux command line tool. Even though it takes some time to get this right, we can acquire new experiential skill as well as gain access to a correct and well-implemented classification for this large file. We start by creating a small test file to use:

```Linux
head -n 1000 311.csv | tail -n 999 > test.csv
```

Note the use of the *head* and *tail* commands. The *head* will include the first line of the file, which, for this file, gives the column names. If we use this command along with sorting the values, the column names will not be preserved as the top line of the file. So we cut them before sorting. If you are using a Linux-based operating system, you can use the *sort* command:

```Linux
sort --field-separator=',' --key=2,3 testsorted.csv
```

Here we identify the field separator and then indicate that we want to sort according to the second and third columns, that is, from the creation date and closing date (which we only know from a previous inspection of the file). We generate the output in a new file, as it wouldn't be very useful to have the standard output.

We can now inspect the sorted file in R, but unfortunately we will find that this does not return a sorted file either. We sort according to a date column (which would be processed as a string, not date-aware). However, the current date formatting starts with the month, so we will end up with a column of dates by month, rather than ordered by overall time, as we see when we review the "sorted" CSV resulting from our previous command.

As we can see, it makes sense to sort by string instead of using a sort by date. In fact, this signals one of the benefits of using proper ISO formatting for dates: you'll still get the correct sorting when sorting as a string, unlike the previous format. This is an example of a very common problem with "found" **time series** data: the available timestamp format may not be the most suitable for **time series** analysis.

We will take a more general approach to see if our available tools can handle this CSV if we only read certain columns. The first question we're interested in about this dataset is how the interval between creating a 311 claim and closing that claim may have varied over time. In this case, I suppose I need just two columns: 'created_date' and 'closed_date'. I will see if it is possible to read just two columns, a tiny portion of all the columns in terms of count and character count (because some columns are gigantic), in my simple notebook.

Now we can read all the rows of data and our further analysis will be on the entire data set, instead of just the first few rows:

Newbies to **Big Data** might be surprised by the fact that 19 million rows isn't that big, but that's how it is. In reality, a small portion of the data is enough to solve a relevant **time series** question. We soon noticed some shockingly incorrect numbers in the *LagTime* column - tens of thousands of days or negative days. We will eliminate these numbers and place a limit on the data, driven in part by a distribution of a random sample of data points:

We discard data with negative lag times, as we lack documentation or domain knowledge to know what they are. We also left out data that we considered to have extreme or unrealistic values, discarding data in which the lag time to close the complaint on line 311 was more than 1,000 days.

Now that we can store all the data of interest in memory at the same time, we can ask questions about the **time series** holistically. However, the question is whether and how the distribution of lag time may have changed over time. We could resort to a rolling or sliding window over the **time series**, but this is computationally heavy; we would have to do a lot of calculations repeatedly as we slid a window over the data. We also want to, as we could imagine the continuation of this project in the current data as it arrives. It would be best not to store this multigigabyte data file indefinitely.

## Online Statistical Analysis of Time Series data

We will use a very simple online quantile estimation tool, called *P-square algorithm* (https://perma.cc/G8LA-7738), but we will modify it so that it is time-aware. The original algorithm assumed that there was a stable distribution from which quantiles were being inferred, but we want to consider the case of distributions that change over time. As with an exponentially weighted moving average, we will add this time awareness by weighting past observations less. And we do this in the same way, introducing a factor to reduce the weights of previous measurements each time a new measurement becomes available.

The algorithm requires a little data recording, making it easier to implement in a more object-oriented programming language. Soon, we will switch from R to Python (I will leave the commands here in Markdown). However, note that we will use preprocessed data from R, as the **data.table** package has considerably better performance with this type of **Big Data** than the tools available in Python.

The version of the P-square algorithm that we will implement will generate a histogram of the values. So, as we create a *PQuantile* object, we allocate a predefined number of bins for our histogram calculations, bin positions, and ongoing observation calculations:

```Python
## Importing libs
import math
import bisect

sign = lambda x: (1, -1)[x < 0]

## Beginning of class definition
class PQuantile:
  def __init__(self, b, disocunt_factor):
    ## Initializing
    self.num_obs = 0
    ## Counts per quantile
    self.n = [i for i in range(self.b+1)]
    self.q = []
    
    ## b is the number of quantiles
    self.b = b
    ## The discount factor defines how we adjust previous counts when new data is available
    self.discount_factor = discount_factor
```

There are two configurable parameters: the number of evenly spaced quantiles to estimate and the discount factor for old observations.

The other members of the class include a total run number of observations (which will be subject to time discounting by the configurable discount factor), the estimated quantiles, and the run count of observations less than or equal to a given quantile value.

There is only one public function, whose role is to accept the next observation. When a new observation arrives, what happens is the result of where we found ourselves at the beginning of the series. For the first *self.b* values, the inputs are accepted and necessarily constitute the quantile estimate. *self.q* is sorted so that its values depict the quantiles.

So, for example, imagine you enter *b = 5* for the number of quantile values you want, and then enter the sequence *2, 8, 1, 4, 3*. At the end of this sequence, *self.q* would equal *[1, 2, 3, 4, 8]*. *self.n*, the counts of values less than or equal to each of the quantiles, would be equal to *[1, 2, 3, 4, 5]*; the value has already been initialized in **__init__"**:

```Python
  def next_obs(self, x):
    if self.num_obs < (self.b + 1):
      self.q.append(x)
      self.q.sort()
      self.num_obs = self.num_obs + 1
    else:
      self.next_obs2(x)
      self.next_obs = self.next_obs2
```

Things get interesting when you have more than *self.b* values. At this point, the code begins to make decisions about how to combine values to estimate quantiles without keeping all the data points stored for repeated analysis. in this case, the P-square algorithm does this with what we call *self.next_obs2*:

```Python
  def next_obs2(self, x):
    ## Discounting the number of observations
    if self.num_obs > self.b * 10:
      corrected_obs = max(self.discount_factor * self.num_obs, self.b)
      self.num_obs = corrected_obs + 1
      self.n = [math.ceil(nn * self.discount_factor) for nn in self.n]
      
      for i in range(len(self.n) - 1):
        if silf.n[i + 1] - self.n[i] == 0:
          self.n[i + 1] = self.n[i + 1] + 1
        elif self.n[i + 1] < self.n[1]:
          ## in practice this does not seem to happen
          self.n[i + 1] = self.n[i] - self.n[1 + i] + 1
    else:
      self.num_obs = self.num+obs + 1
      
    k = bisect.bisect_left(self.q, x)
    if k is 0:
      self.q[0] = x
    elif k is self.b + 1:
      self.q[-1] = x
      k = self.b
    if k is not 0:
      k = k - 1
    
    self.n[(k + 1):(self.b + 1)] = [self.n[i] + 1 for i in range((k + 1), (self.b + 1))]
    
    for i in range(1, self.b):
      np = (i) * (self.num_obs - 1) / (self.b)
      d = np - self.n[i]
      if (d >= 1 and (self.n[i + 1] - self.n[i]) > 1):
        self._update_val(i, d)
      elif (d <= and (self.n[i + 1] - self.n[i]) < -1):
        self._update_val(i, d)
```

Preferably, the ith auantille value should be evenly spaced so that the total observations *i/b* × are smaller than it. If this is not the case, the marker is shifted to a position to the left or right, and its associated quantile value is modified using a formula derived from the assumption of a local parabolic formula of the histogram. This formula establishes the previously indicated criteria for sizing the variable *d*, in order to determine whether a stipulated quantile and count value should be adjusted.

If the value has to be adjusted, we must make another decision: whether parabolic or linear adjustment is appropriate. We implement this in the code below. For more details, check out the original article (https://perma.cc/G8LA-77#*). This article is excellent because of the accessible calculations used and also because it provides very clear instructions on how to implement the method and then test its implementation:

```Python
## General update
## As we can see, 'self.q' and 'self.n' are updated according to the position of a shifted quantile
def _update_val(self, i, d):
  d  = sign(d)
  qp = self._adjust_parabolic(i, d)
  if self.q[i] < qp < self.q[i+1]:
    self.q[i] = qp
  else:
    self.q[i] = self._adjust_linear(i, d)
  self.n[i] = self.n[i] + d
  
## This is the main update method
def _adjust_parabolic(self, i, d):
  new_val = self.q[i]
  m1      = d/(self.n[i+1] - self.n[i-1])
  s1      = (self.n[9] - self.n[i-1] + d) * (self.q[i+1] - self.q[i]) / (self.n[i+1] - self.n[i])
  s2      = (self.n[i+1] - self.n[i] - d)
  
## Backup of linear fit when parabolic nap conditions are met
def _adjust_linear(self, i, d):
  new_val = self.q[i]
  new_val = new_val + d * (self.q[i + d] - self.q[i]) / (self.n[i+d] - self.n[i])
```

To get a sense of the simplicity of this method, all the class code is listed here, in one place:

```Python
class PQuantile:
  ## Initializing
  def __init__(self, b, discount_facotr):
    self.num_obs.        = 0
    self.b               = b
    self.discount_factor = discount_factor
    self.n               = [i for i in range(self.b+1)]
    self.q               = []
    
  ## Data intake
  def next_obs(self, x):
    if self.num_obs < (self.b + 1):
      self.q.append(x)
      self.q.sort()
      self.num_obs = self.num_obs + 1
    else:
      self.next_obs2(x)
      self.next_obs = self.next_obs2
  
  def next_obs2(self, x):
    ## Descontando o numero de observacoes
    if self.num_obs > self.b * 10:
      corrected_obs = max(self.discount_factor * self.num_obs, self.b)
      self.num_obs  = corrected_obs + 1
      self.n = [math.ceil(nn * self.disocunt_factor) for nn in self.n]
      
      for i in range(len(self.n) - 1):
        if self.n[i + 1] - self.n[i] == 0:
          self.n[i + 1] = self.[i + 1] + 1
        elif self.n[i + 1] < self.n[1]:
          ## na pratica, isso nao parece acontecer
          self.n[i + 1] = self.n[i] - self.n[i + 1] + 1
    else:
      self.num_obs = self.num_obs + 1
    
    k = bisect.bisect_left(self.q, x)
    if k is 0:
      self.q[0] = x
    elif k is self.b + 1:
      self.q[-1] = x
      k = self.b
    if k is not 0:
      k = k - 1
    
    self.n[(k+1):(self.b+1)] = [self.n[i] + 1 for i in range((k+1), (self.b+1))]
    for i in range(1, self.b):
      np = (i)*(self.num_obs - 1)/(self.b)
      d = np - self.n[i]
      if (d >= -1 and (self.n[i+1] - self.n[i]) > 1):
        self._update_val(i, d)
      elif (d <= -1 and (self.n[i=1] - self.n[i]) < -1):
        self._update_val(i, d)
        
  ## Ajustes do histograma
  def _update_val(self, i, d):
    d  = sign(d)
    qp = self._adjust_parabolic(i, d)
    if self.q[i] < qp < self.q[i+1]:
      self.q[i] = qp
    else:
      self.q[i] = self._adjust_linear(i, d)
    self.n[o] = self.n[i] + d
    
  def _adjust_parabolic(self, i, d):
    new_val = self.q[i]
    m1 = d/(self.n[i+1] - self.n[i-1])
    s1 = (self.n[i] - self.n[i-1] + d) * (self.q[i+1] - self.q[i]) / (self.n[i+1]) - self.n[i]
    s2 = (self.n[i+1] - self.n[i] - d) * (self.q[i] - self.q[i-1]) / (self.n[i]) - self.n[i-1]
    new_val = new_val + m1 * (s1 + s2)
    return new_val
    
  def _adjust_linear(self, i, d):
    new_val = self.q[i]
    new_val = new_val + d * (self.q[i + d] - self.q[i]) / (self.n[i + d] - self.n[i])
    return new_val
```

Now that we have this time-oriented method, we should be convinced that it works reasonably well with a test example. first we will try to sample data points from one distribution and then suddenly switch to another. In each case, we will be sampling for the 40th percentile, and although we have relied on the 10-point histogram setting, we will be maintaining a histogram that indicates the 10th, 20th, ..., 90th, and 100th percentile. This helps us, as we will have a very detailed description of the changing distribution. For this test example, we only focus on the 40th percentile *(qt.q[4])*:

```Python
qt = PQuantile(10, 0.3)
qt_ests = []

for _ in range (100):
  b.next_obs(uniform())
  if len(b.q) > 10:
    qt_ests.append(qt.q[4])
for _ in range (100):
  b.next_obs(uniform(low = 0.9))
  qt.ests.append(qt.q[4])
  
plt.plot(qt_ests)
```

On the other hand, we observed slower adoption in the variable quantile in the case of a higher discount factor:

```Python
qt = PQuantile(10, 0.8)
qt_ests = []

for _ in range (100):
  b.next_obs(uniform())
  if len(b.q) > 10:
    qt_ests.append(qt.q[4])
for _ in range (100):
  b.next_obs(uniform(low = 0.9))
  qt.ests.append(qt.q[4])
  
plt.plot(qt_ests)
```

Now, we will apply this rolling quantile to a subset of our data. I didn't do this for the entire dataset, not because of the computational challenge, but because it was too cumbersome for the notebook to graph the recorded quantiles:

```Python
import numpy as np
nrows = 1000000
qt_est1 = np.zeros(nrwos)
qt_est2 = np.zeros(nrwos)
qt_est3 = np.zeros(nrwos)
qt_est4 = np.zeros(nrwos)
qt_est5 = np.zeros(nrwos)
qt_est6 = np.zeros(nrwos)
qt_est7 = np.zeros(nrwos)
qt_est8 = np.zeros(nrwos)
qt_est9 = np.zeros(nrwos)
for idx, val in enumerate(df.LagTime[:nrows]):
  qt.next_obs(val)
  if len(qt.q) > 10:
    qt_est1[idx] = qt.q[1]
    qt_est1[idx] = qt.q[2]
    qt_est1[idx] = qt.q[3]
    qt_est1[idx] = qt.q[4]
    qt_est1[idx] = qt.q[5]
    qt_est1[idx] = qt.q[6]
    qt_est1[idx] = qt.q[7]
    qt_est1[idx] = qt.q[8]
    qt_est1[idx] = qt.q[9]

plt.plot(qt_est9, color = 'red')
plt.plot(qt_est7, color = 'pink')
plt.plot(qt_est5, color = 'bluw')
plt.plot(qt_est3, color = 'gray')
plt.plot(qt_est2, color = 'orange')
```

The graph generated from the code above demonstrates the 90th, 70th, 50th, 30th, 20th percentile values, plotted over time for the first 100 thousand rows in the data set when sorted by end date. Because they were sorted by closing date, it is likely that many quickly resolved 311 complaints were frontloaded, which explains the much lower quantile estimates in the first portion of the data set.

Mas a distribuicao mudou? Visualmente parece que sim, por alguns motivos. Um One of these is left censoring, briefly described, which reflects how we classify and select data. The fact that we sort the data via the 'closed_date' column, coupled with the fact that this data set does not appear to have an infinite lookahead (i.e. presumably 311 complaints made before a given date have not reached this system), makes the lag times we see in the start dates seem longer. Roughly speaking, this apparent change over time is simply an artifact of our incomplete data (and an incomplete underlying dataset) coupled with our choice of classification.

In contrast, we can see features that suggest changes in distributions over time. Apparently, there are peaks and valleys in the quantile curve estimates, and we can even consider the existence of periodic behavior in our curves, as there may be predictable times when quantile values ​​rise and fall due to exogenous organizational factors (perhaps an effort to close claims before the end of the month, or certain funding cycles that increase the number of workers available to close claims at certain times of the year).

Since we now have preliminary results, the best option would be to determine some significant dates and try to cross-reference them with any institutional facts we can determine about the pace of work. We should also run simulations to evaluate how we think left censoring should impact the initial quantile estimates in different scenarios. Thus, we could have a better qualitative and quantitative understanding of the unknown aspects of our system, and this information would be very useful in making a final determination about whether the distribution of resolution times is evolving over time and, if so, how.

Let's say we follow these steps - what then? A methodology would be needed to compare distributions for similarity differences in which only the quantiles of the distribution. rather than all sample points were available. One way to do this would be to run simulations/bootstrap throughout the process. This would lead to an answer where we could fully express and control the assumptions that went into the model by programming our simulations. In fact, many statistical approaches that perform these purchases also focus on bootstrap methods.

---

##### Left censoring, right censoring, and interval censoring

For **time series analysis** and related activities such as survival analysis, we must be aware of how our selection, classification, or presentation of data may impact our analysis, as we determined whether this is the case here, where earlier time periods appeared to have a stricter distribution than later ones. This is probably due to the fact that we classified the data by closing date, a form of *left censoring*, which means that the "event of interest" already occurred before the start of the "study". Here, our event of interest is the start and end dates of complaints and we are limited by the start date of the data set. On the other hand, the related concept of *right censoring* means that the event of interest would occur after an event has ended. Lastly, *interval censoring* concerns situations where we only have an inaccurate record, such as an interval, for when an event occurred, which will necessarily impact our analysis.

---

### Pending Issues

Our visualization suggests new queries. One concerns the possibility of cyclical or seasonal behavior. Apparently, there are periodic impacts in all estimated quantiles. We may consider further investigation into this, and we have several options:

- we could try to adjust the harmonics (sines and cosines) to these quantile curves and analyze whether a common periodicity emerged;
- we could model the quantile itself as an ARIMA or SARIMA process and look for evidence of seasonality. This would also result in preliminary steps, such as exploring the ACF and PACF of the curves that we would model as **time series**;
- we could ask the agency that manages the 311 service for more information and see if they recognize any periodic behavior induced by their organizational structure and operational procedures.

In addition to the regular behavior, we can also see a jump in quantile values just below the index location = 70,000. Since all quantiles fluctuated, it seems likely that this is due to just one or a set of outliers. Let's look at ways to investigate:

- go back to the raw data from this time period and examine what features might provide an explanation. Has there been an increase in 311 hotline complaints? Or a spike in a specific type of query that tends to take longer to resolve?
- we also thought about reanalyzing the raw data to recover the approximate data of this jump in quantiles and cross-referencing it with local news, preferably with the help of someone who can point in the right direction. Help from someone at the agency, or someone who understands municipal government, can be very helpful. But it is also possible that this data corresponds to a major event in New York that would explain the jump, such as the passage of Hurricane Sandy in 2012.

### Other improvements

We could make our algorithm even more time aware. Our modification to the P-square algorithm disregards previous observations, assuming that all observations are evenly spaced. This manifests itself in the fact that the next observation entry does not have a timestamp, and the same discount factor is always applied. We could create a more flexible algorithm using the change in timestamp for old information compared to new information, so that the discount depends on the change in time from the last measured update. This would also be more accurate for our row 311 dataset.
We could also consider other ways of estimating quantiles over time, whether with an online or window measurement. As the importance of online data is growing - and especially when it comes to online **Big Data** - there is a variety of emerging research on this topic. Statistical and machine learning approaches have addressed this in recent years, and there are a good number of academic articles available.