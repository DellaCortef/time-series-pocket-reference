---
title: "Government Uses"
author: "Della"
date: "2025-02-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

For several reasons, **time series** analysis is pertinent and invaluable for government uses. First, governments, large and small, are custodians of some of the world's most important time series data, including the US jobs report, ocean temperature data, and local crime statistics. Second, governments, by definition, provide some of the most essential services we all depend on. Therefore, they need to be reasonably skillful forecasters of demand if they do not want to overspend or understaff these services. Thus, all aspects of **time series** are relevant to government objectives: storage, cleaning, exploration and forecasting.

As we mentioned earlier, when analyzing "found" **time series**, a relatively high percentage of all government data can look a lot like **time series** data if we restructure it a bit. In general, most government datasets are the result of continuous data collection, not a single chunk of time. However, government datasets can be scary for several reasons:

- inconsistent data records (due to organizational constraints or political forces changing over time);
- shady or confusing data practices;
- gigantic set of data with relatively low information content.

However, it can be very beneficial to analyze government datasets for both intellectual interest and many practical purposes. Now, we will explore a government dataset consisting of all complaints filed in New York City from 2010 to present (https://perma.cc/BXF6-BZ4X) through a city hotline that can be accessed by dialing 311. We will cover the following topics:

- interesting sources of government data, including the one we will analyze;
- deal with gigantic plain text data files;
- online/continuous statistical analysis of large datasets and other options to analyze data without keeping it in memory.

## Obtaining Government Data

Government datasets that fall into the “found data” category can be a nightmare from a data consistency standpoint. These datasets, although timestamped, are generally made available for open source initiatives and not for a specific **time series** purpose. Often, there is little or no information available about data timestamp conventions or other recording conventions. It can be difficult to confirm that underlying recordkeeping practices were consistent.

But if you are curious or want to be the first to identify interesting temporal characteristics in human behaviors related to government activities, you are lucky to live in the open source government era. In recent years, regardless of government hierarchies, many governments have strived hard to make their **time series** data transparent to the public. Below, we'll look at just a few examples of where we can get open government data with a **time series** component:

- monthly hospital data (https://perma.cc/4TR3-84WA) from the UK National Health Service. This dataset is very time series aware: it includes a tab called "MAR timeseries" and describes the recording conventions and how they have evolved over time;
- Jamaica's open data portal also includes an assessment and recognition of **time series** data, such as its timestamped dataset on Chikungunya cases (https://perma.cc/4RCP-VMY6) from 2014 and the associated data report (https://perma.cc/QPR6-WNMJ), which includes an animation (i.e. a **time series** visualization) and an epidemiological curve;
- Singapore's open data portal (https://perma.cc/N9W4-ZDM8) presents extensive data sets and discloses the **time series** nature of some of this data, including two **time series** charts.

## Exploring Big Time Series Data

When the data is large enough, we will not be able to put it into memory. How big the data needs to be before reaching this limit will depend on the hardware we are using. Sooner or later, we'll need to understand how to iterate through your data, one manageable piece at a time. Those familiar with deep learning have probably already done this, especially if they have worked in image processing. Deep learning frameworks have made available Python iterators that work their way through a dataset, and these datasets are stored in specific directories, each with many files.

At the time of testing, the 311 dataset was over 3 gigabytes in CSV format. There was no way to open this on the machine, so the first idea was to use the standard Unix system options, such as *head*. Unfortunately, what was displayed was already so large as to be impossible to manage in a Unix command-line interface:

```Linux
head 311.csv
```

Although the content is heavy, this preview was enough to show that there were multiple timestamps as well as other interesting and orderly information like geographic coordinates. Of course, the data is vast, so we will need to be able to manipulate this information to obtain the columns we want.

Even if you are new to Linux, you can easily learn simple command-line tools that can provide you with useful information. We can get a row count from the CSV file to get a sense of the scale we are analyzing, that is, how many data points we have:

```Linux
wc -l 311.csv
```

We can see that NYC has received 1,202,182 complaints through its 311 hotline. We use data from 2024 to today.

Armed with this knowledge, we will use R's *data.table*, as we know that its *fread()* function allows partial reading of files (https://perma.cc/ZHN9-5HD3). Note the *nrows()* and *skip()* parameter part. We also know that *data.table* is extremely efficient when dealing with large data sets. We can use them to obtain initial information, like this:

```R
# Install packages
install.packages("data.table")

# Load packages
library(data.table)

# Setting the dataset path
setwd('/Users/dellacorte/py-projects/data-science/time-series-pocket-reference/datasets/')

df = fread("311.csv", skip = 0, nrows = 10)
colnames(df)
```

Just read ten lines and we can see the column names. Of all the advantages I've listed of NoSQL approaches to **time series** data, it can be nice, in a large dataset, to know the column names from the beginning. Of course, there are alternative solutions with NoSQL data, but most require a little effort on the part of the user, as things don't flow automatically. Several columns suggest useful information:

  "Created Date"
  
  "Closed Date"
  
  "Due Date"
  
  "Resolution Action Updated Date"
  
These columns will likely be of type character before we convert them, but once we do a conversion to a POSIXct type, we can examine what the time interval between dates looks like:

```R
df[, created_date := as.POSIXct(created_date, format = "%m/%d/%Y %I:%M:%S %p")]
```

In the format string, we need to use %I for the time, as it is only expressed in the format 01-12, and %p because timestamps are dammed as AM/PM. To get a sense of how spaced these dates tend to be, especially when it comes to when a claim is opened or closed, we'll load more rows and look at the distribution that I'll call the *claim period* (i.e., the interval between opening and closing a claim):

```R
> summary(df$duration_days)
```

---
title: "Summary Statistics Table"
output: html_document
---

```{r, echo=FALSE, message=FALSE}
# Load required libraries
library(knitr)
library(kableExtra)

# Create summary statistics table
summary_stats <- data.frame(
  Statistic = c("Min.", "1st Qu.", "Median", "Mean", "3rd Qu.", "Max.", "NA's"),
  Value = c(-66.00, 0.06, 0.38, 3.34, 2.20, 114.62, 99777)
)

# Render the table in HTML format
kable(summary_stats, format = "html", caption = "Summary Statistics of Duration in Days") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = FALSE)
```

As we can see, it is a wide distribution. More impressive than the long years of waiting for some complaints to be closed is the fact that some of them have negative, even extremely negative, results between their creation and the closing date. If the negative time was around -365 days, we could imagine a data entry problem, but this seems less likely with numbers like -66 days. It's the kind of problem we need to look at. We can identify another one, when we get a creation date range:

```R
> range(df$created_date)
[1] "2024-10-01 16:09:24 -03" "2025-01-26 03:33:15 -03"
```

Given the size of this CSV file (three months or so) and the fact that it must be constantly updated, it is a wonder that the first few lines are not from 2010, which should mark the first datasets. We expected the CSV to be continually appended. And even more surprising is the fact that the dates of 2024 are in the first lines. This suggests that we cannot determine the date ordering of the data in the file. We can visualize the date distribution from one line to the next by plotting a line graph on the index against the date:

![](https://github.com/DellaCortef/time-series-pocket-reference/blob/main/government-uses-of-time-series/images/cumulative-record-count-over-time.png?raw=true)

There is no way to avoid this problem. If we want to understand how behavior changes over time, we will have to deal with unordered data. But we have some options.

### Use Upsampling and aggregate Data as we iterate

Uma dessas opções é fazer o upsampling dos dados à medida que iteramos por eles para construir uma **série temporal** condensada com estatísticas agregadas. Poderíamos escolher uma resolução e funções de agregação desde o início de nossa análise e, em seguida, calculá-las à medida que iteramos. Depois, poderíamos classificar nossos resultados no final de nossa análise. Seria como ter, digamos, um dicionário/lista de todas as datas de 2024 (ou antes, 2010 por exemplo) até o presente e, em seguida, bastaria acrescentar a data apropriada para cada linha. Este método geraria uma lista/dicionário com um número relativamente pequeno de entradas e poderíamos, assim, classificá-las no final com base na data.

A vantagem é que isso seria relativamente simples de codificar e seria uma maneira de combinar limpeza, exploração. análise de dados em uma etapa exploratória. A desvantagem é que os dados detalhados ainda estariam perdidos no limbo do arquivo não classificados de modo que, se houvesse um período específico de interesse, teríamos que pesquisar todo o arquivo não classificado a fim de encontrar todas as entradas relevantes.

### Classificando os Dados

Outra opção é classificar os dados. É uma tarefa hérculea, visto o tamanho gigantesco do arquivo e as datas relativamente desordenadas que pudemos observar. Mesmo nos dados de 2014, não parece que as datas estejam ordenadas. Por isso, não temos indicações de que podemos confiar em quaisquer faa de dados. Ou seja, devemos considerar isso uma pilha de dados em ordem aleatória.

Fazer a classificação do arquivo completo exigiria bastante memória, mas isso pode vaer a pena por dois motivos. Um deles é que basta classificar uma vez, e depois podemos salvar nossos resiltados para qualquer análise posterior que quisermos fazer. O outro é que podemos ter o nível total de detalhes preservado. Desse modo, se identificarmos períodos de tempo específicos de interesse em nossa análise, podemos examinar os dados em todos os seus detalhes para entender o que está acontecendo. Em termos concretos, e pensando como poderíamos fazer isso, temos algumas opções:

- o Linux tem uma ferramenta de linha de comando para classificação;
- a maioria dos bancos de dados pode classificar dados, assim podemos transferi-los para um banco de dados e deixar que o banco se encarregue desse processo;
- poderíamos criar nosso próprio algoritmo de classificação e implementá-lo. É necessário formular algo que não consuma uma enorme quantidade de memória. As probabilidades de nossos empenhos se compararem ao que está dispon;ivel nas opções de classificação pré-empacotadas são mínimas.

Optamos por usar uma ferramenta de linha de comando do Linux. Ainda que demore algum tempo para fazer isso direito, podemos adquirir uma nova aptidão vivencial, bem como obter acesso a uma classificação correta e bem implementada para esse arquivo grande. Começamos criando um pequeno arquivo de teste para usar:

```Linux
head -n 1000 311.csv | tail -n 999 > test.csv
```

Repare o uso dos comandas *head* e *tail*. O *head* incluirá a primeira linha do arquivo, que, para esse arquivo, fornece os nomes das colunas. Se usarmos esse comando junto com a classificação dos valores, os nomes das colunas não serão preservados como a linha superior do arquivo. Assim, as cortamos antes de classificar. Caso esteja usando um sistema operacional baseado em Linux, poderá utilizar o comando *sort*:

```Linux
sort --field-separator=',' --key=2,3 testsorted.csv
```

Aqui, identificamos o field separator e, em seguida, indicamos que queremos classificar de acordo com a segunda e terceira colunas, ou seja, a partir da data de criação e data de encerramento (que sósabemos por meio de uma inspeção anterior do arquivo). Geramos a saída em um novo arquivo, pois não seri lá muito útil ter a saída-padrão.

Agora podemos inspecionar o arquivo classificado em R, mas infelizmente descobriremos que isso também não retorna um arquivo classificado. Classificamos de acordo com uma coluna de data (que seria processada como uma string, e não com reconhecimento de data). Entretanto, a formatação atual das datas começa com o mês, assim acabaremos com uma coluna de datas por mês, em vez de ordenadas pelo tempo geral, como vemos quando revisamos o CSV "classificado" resultante de nosso comando anterior.

Conforme podemos ver, faz sentido a classificação por string em vez de usar uma classificação por data. Na verdade, isso sinaliza um dos benefícios de usar a formatação adequada ISO para datas: você ainda obterá a classificação correta ao classificar como um string, ao contrário do formato anterior. Esse é um exemplo de um problema muito comum com dados de **séreis temporais** "encontrados": o formato timestamp disponível pode não ser o mais adequado para a análise de **séries temporais**.

Adotaremos uma abordagem mais geral para ver se nossas ferramentas disponíveis podem idar com esse CSV caso façamos a leitura somente de determinadas colunas. A primeira pergunta que nos interessa sobre esse conjunto de daods é como o intervalo entre a criação de uma reclamação no 311 e o encerramento dessa reclamação pode ter variado ao longo do tempo. Meste caso, suponho que preciso de apenas duas colunas: 'created_date' e 'closed_date'. verei se é possível ler apenas duas colunas, uma parcela ínfima de todas as colunas em termos de contagem e contagem de caracteres (porque algumas colunas são gigantes), em meu simples notebook. 

Agora podemos ler todas as linhas dos dados e nossas análise posterior será no conjunto de dados completo, em vez de analisar somente as primeiras linhas:

Os principiantes em **Big Data** podem se supreender com o fato de que 19 milhões de linhas não é grande cposa, mas é assim mesmo. Na realidade, basta uma pequena parcela dos dados para solucionar uma questão relevante de **série temporal**. Logo percebemos alguns numeros espantosamente incorretos na coluna *LagTime* - dezenas de milhares de dias ou dias negativos. Eliminaremos esses numeros e colocaremos um limite nos dados, orientados, em parte, por uma distribuição de uma amostra aleatória de pontos de dados:

