---
title: "Government Uses"
author: "Della"
date: "2025-02-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

For several reasons, **time series** analysis is pertinent and invaluable for government uses. First, governments, large and small, are custodians of some of the world's most important time series data, including the US jobs report, ocean temperature data, and local crime statistics. Second, governments, by definition, provide some of the most essential services we all depend on. Therefore, they need to be reasonably skillful forecasters of demand if they do not want to overspend or understaff these services. Thus, all aspects of **time series** are relevant to government objectives: storage, cleaning, exploration and forecasting.

As we mentioned earlier, when analyzing "found" **time series**, a relatively high percentage of all government data can look a lot like **time series** data if we restructure it a bit. In general, most government datasets are the result of continuous data collection, not a single chunk of time. However, government datasets can be scary for several reasons:

- inconsistent data records (due to organizational constraints or political forces changing over time);
- shady or confusing data practices;
- gigantic set of data with relatively low information content.

However, it can be very beneficial to analyze government datasets for both intellectual interest and many practical purposes. Now, we will explore a government dataset consisting of all complaints filed in New York City from 2010 to present (https://perma.cc/BXF6-BZ4X) through a city hotline that can be accessed by dialing 311. We will cover the following topics:

- interesting sources of government data, including the one we will analyze;
- deal with gigantic plain text data files;
- online/continuous statistical analysis of large datasets and other options to analyze data without keeping it in memory.

## Obtaining Government Data

Government datasets that fall into the “found data” category can be a nightmare from a data consistency standpoint. These datasets, although timestamped, are generally made available for open source initiatives and not for a specific **time series** purpose. Often, there is little or no information available about data timestamp conventions or other recording conventions. It can be difficult to confirm that underlying recordkeeping practices were consistent.

But if you are curious or want to be the first to identify interesting temporal characteristics in human behaviors related to government activities, you are lucky to live in the open source government era. In recent years, regardless of government hierarchies, many governments have strived hard to make their **time series** data transparent to the public. Below, we'll look at just a few examples of where we can get open government data with a **time series** component:

- monthly hospital data (https://perma.cc/4TR3-84WA) from the UK National Health Service. This dataset is very time series aware: it includes a tab called "MAR timeseries" and describes the recording conventions and how they have evolved over time;
- Jamaica's open data portal also includes an assessment and recognition of **time series** data, such as its timestamped dataset on Chikungunya cases (https://perma.cc/4RCP-VMY6) from 2014 and the associated data report (https://perma.cc/QPR6-WNMJ), which includes an animation (i.e. a **time series** visualization) and an epidemiological curve;
- Singapore's open data portal (https://perma.cc/N9W4-ZDM8) presents extensive data sets and discloses the **time series** nature of some of this data, including two **time series** charts.

## Exploring Big Time Series Data

When the data is large enough, we will not be able to put it into memory. How big the data needs to be before reaching this limit will depend on the hardware we are using. Sooner or later, we'll need to understand how to iterate through your data, one manageable piece at a time. Those familiar with deep learning have probably already done this, especially if they have worked in image processing. Deep learning frameworks have made available Python iterators that work their way through a dataset, and these datasets are stored in specific directories, each with many files.

At the time of testing, the 311 dataset was over 3 gigabytes in CSV format. There was no way to open this on the machine, so the first idea was to use the standard Unix system options, such as *head*. Unfortunately, what was displayed was already so large as to be impossible to manage in a Unix command-line interface:

```Linux
head 311.csv
```

Although the content is heavy, this preview was enough to show that there were multiple timestamps as well as other interesting and orderly information like geographic coordinates. Of course, the data is vast, so we will need to be able to manipulate this information to obtain the columns we want.

Even if you are new to Linux, you can easily learn simple command-line tools that can provide you with useful information. We can get a row count from the CSV file to get a sense of the scale we are analyzing, that is, how many data points we have:

```Linux
wc -l 311.csv
```

We can see that NYC has received 1,202,182 complaints through its 311 hotline. We use data from 2024 to today.

Armed with this knowledge, we will use R's *data.table*, as we know that its *fread()* function allows partial reading of files (https://perma.cc/ZHN9-5HD3). Note the *nrows()* and *skip()* parameter part. We also know that *data.table* is extremely efficient when dealing with large data sets. We can use them to obtain initial information, like this:

```R
# Install packages
install.packages("data.table")

# Load packages
library(data.table)

# Setting the dataset path
setwd('/Users/dellacorte/py-projects/data-science/time-series-pocket-reference/datasets/')

df = fread("311.csv", skip = 0, nrows = 10)
colnames(df)
```

Just read ten lines and we can see the column names. Of all the advantages I've listed of NoSQL approaches to **time series** data, it can be nice, in a large dataset, to know the column names from the beginning. Of course, there are alternative solutions with NoSQL data, but most require a little effort on the part of the user, as things don't flow automatically. Several columns suggest useful information:

  "Created Date"
  
  "Closed Date"
  
  "Due Date"
  
  "Resolution Action Updated Date"
  
These columns will likely be of type character before we convert them, but once we do a conversion to a POSIXct type, we can examine what the time interval between dates looks like:

```R
df[, created_date := as.POSIXct(created_date, format = "%m/%d/%Y %I:%M:%S %p")]
```

In the format string, we need to use %I for the time, as it is only expressed in the format 01-12, and %p because timestamps are dammed as AM/PM. To get a sense of how spaced these dates tend to be, especially when it comes to when a claim is opened or closed, we'll load more rows and look at the distribution that I'll call the *claim period* (i.e., the interval between opening and closing a claim):

```R
> summary(df$duration_days)
```

---
title: "Summary Statistics Table"
output: html_document
---

```{r, echo=FALSE, message=FALSE}
# Load required libraries
library(knitr)
library(kableExtra)

# Create summary statistics table
summary_stats <- data.frame(
  Statistic = c("Min.", "1st Qu.", "Median", "Mean", "3rd Qu.", "Max.", "NA's"),
  Value = c(-66.00, 0.06, 0.38, 3.34, 2.20, 114.62, 99777)
)

# Render the table in HTML format
kable(summary_stats, format = "html", caption = "Summary Statistics of Duration in Days") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = FALSE)
```

As we can see, it is a wide distribution. More impressive than the long years of waiting for some complaints to be closed is the fact that some of them have negative, even extremely negative, results between their creation and the closing date. If the negative time was around -365 days, we could imagine a data entry problem, but this seems less likely with numbers like -66 days. It's the kind of problem we need to look at. We can identify another one, when we get a creation date range:

```R
> range(df$created_date)
[1] "2024-10-01 16:09:24 -03" "2025-01-26 03:33:15 -03"
```

Given the size of this CSV file (three months or so) and the fact that it must be constantly updated, it is a wonder that the first few lines are not from 2010, which should mark the first datasets. We expected the CSV to be continually appended. And even more surprising is the fact that the dates of 2024 are in the first lines. This suggests that we cannot determine the date ordering of the data in the file. We can visualize the date distribution from one line to the next by plotting a line graph on the index against the date:

![](https://github.com/DellaCortef/time-series-pocket-reference/blob/main/government-uses-of-time-series/images/cumulative-record-count-over-time.png?raw=true)

There is no way to avoid this problem. If we want to understand how behavior changes over time, we will have to deal with unordered data. But we have some options.

### Use Upsampling and aggregate Data as we iterate