---
title: "Government Uses"
author: "Della"
date: "2025-02-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

For several reasons, **time series** analysis is pertinent and invaluable for government uses. First, governments, large and small, are custodians of some of the world's most important time series data, including the US jobs report, ocean temperature data, and local crime statistics. Second, governments, by definition, provide some of the most essential services we all depend on. Therefore, they need to be reasonably skillful forecasters of demand if they do not want to overspend or understaff these services. Thus, all aspects of **time series** are relevant to government objectives: storage, cleaning, exploration and forecasting.

As we mentioned earlier, when analyzing "found" **time series**, a relatively high percentage of all government data can look a lot like **time series** data if we restructure it a bit. In general, most government datasets are the result of continuous data collection, not a single chunk of time. However, government datasets can be scary for several reasons:

- inconsistent data records (due to organizational constraints or political forces changing over time);
- shady or confusing data practices;
- gigantic set of data with relatively low information content.

However, it can be very beneficial to analyze government datasets for both intellectual interest and many practical purposes. Now, we will explore a government dataset consisting of all complaints filed in New York City from 2010 to present (https://perma.cc/BXF6-BZ4X) through a city hotline that can be accessed by dialing 311. We will cover the following topics:

- interesting sources of government data, including the one we will analyze;
- deal with gigantic plain text data files;
- online/continuous statistical analysis of large datasets and other options to analyze data without keeping it in memory.

## Obtaining Government Data

Government datasets that fall into the “found data” category can be a nightmare from a data consistency standpoint. These datasets, although timestamped, are generally made available for open source initiatives and not for a specific **time series** purpose. Often, there is little or no information available about data timestamp conventions or other recording conventions. It can be difficult to confirm that underlying recordkeeping practices were consistent.

But if you are curious or want to be the first to identify interesting temporal characteristics in human behaviors related to government activities, you are lucky to live in the open source government era. In recent years, regardless of government hierarchies, many governments have strived hard to make their **time series** data transparent to the public. Below, we'll look at just a few examples of where we can get open government data with a **time series** component:

- monthly hospital data (https://perma.cc/4TR3-84WA) from the UK National Health Service. This dataset is very time series aware: it includes a tab called "MAR timeseries" and describes the recording conventions and how they have evolved over time;
- Jamaica's open data portal also includes an assessment and recognition of **time series** data, such as its timestamped dataset on Chikungunya cases (https://perma.cc/4RCP-VMY6) from 2014 and the associated data report (https://perma.cc/QPR6-WNMJ), which includes an animation (i.e. a **time series** visualization) and an epidemiological curve;
- Singapore's open data portal (https://perma.cc/N9W4-ZDM8) presents extensive data sets and discloses the **time series** nature of some of this data, including two **time series** charts.

## Exploring Big Time Series Data

When the data is large enough, we will not be able to put it into memory. How big the data needs to be before reaching this limit will depend on the hardware we are using. Sooner or later, we'll need to understand how to iterate through your data, one manageable piece at a time. Those familiar with deep learning have probably already done this, especially if they have worked in image processing. Deep learning frameworks have made available Python iterators that work their way through a dataset, and these datasets are stored in specific directories, each with many files.

At the time of testing, the 311 dataset was over 3 gigabytes in CSV format. There was no way to open this on the machine, so the first idea was to use the standard Unix system options, such as *head*. Unfortunately, what was displayed was already so large as to be impossible to manage in a Unix command-line interface:

```Linux
head 311.csv
```

Although the content is heavy, this preview was enough to show that there were multiple timestamps as well as other interesting and orderly information like geographic coordinates. Of course, the data is vast, so we will need to be able to manipulate this information to obtain the columns we want.

Even if you are new to Linux, you can easily learn simple command-line tools that can provide you with useful information. We can get a row count from the CSV file to get a sense of the scale we are analyzing, that is, how many data points we have:

```Linux
wc -l 311.csv
```

We can see that NYC has received 1,202,182 complaints through its 311 hotline. We use data from 2024 to today.

Armed with this knowledge, we will use R's *data.table*, as we know that its *fread()* function allows partial reading of files (https://perma.cc/ZHN9-5HD3). Note the *nrows()* and *skip()* parameter part. We also know that *data.table* is extremely efficient when dealing with large data sets. We can use them to obtain initial information, like this:

```R
# Install packages
install.packages("data.table")

# Load packages
library(data.table)

# Setting the dataset path
setwd('/Users/dellacorte/py-projects/data-science/time-series-pocket-reference/datasets/')

df = fread("311.csv", skip = 0, nrows = 10)
colnames(df)
```

Just read ten lines and we can see the column names. Of all the advantages I've listed of NoSQL approaches to **time series** data, it can be nice, in a large dataset, to know the column names from the beginning. Of course, there are alternative solutions with NoSQL data, but most require a little effort on the part of the user, as things don't flow automatically. Several columns suggest useful information:

  "Created Date"
  
  "Closed Date"
  
  "Due Date"
  
  "Resolution Action Updated Date"
  
These columns will likely be of type character before we convert them, but once we do a conversion to a POSIXct type, we can examine what the time interval between dates looks like:

```R
df[, created_date := as.POSIXct(created_date, format = "%m/%d/%Y %I:%M:%S %p")]
```

In the format string, we need to use %I for the time, as it is only expressed in the format 01-12, and %p because timestamps are dammed as AM/PM. To get a sense of how spaced these dates tend to be, especially when it comes to when a claim is opened or closed, we'll load more rows and look at the distribution that I'll call the *claim period* (i.e., the interval between opening and closing a claim):

```R
> summary(df$duration_days)
```

---
title: "Summary Statistics Table"
output: html_document
---

```{r, echo=FALSE, message=FALSE}
# Load required libraries
library(knitr)
library(kableExtra)

# Create summary statistics table
summary_stats <- data.frame(
  Statistic = c("Min.", "1st Qu.", "Median", "Mean", "3rd Qu.", "Max.", "NA's"),
  Value = c(-66.00, 0.06, 0.38, 3.34, 2.20, 114.62, 99777)
)

# Render the table in HTML format
kable(summary_stats, format = "html", caption = "Summary Statistics of Duration in Days") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = FALSE)
```

As we can see, it is a wide distribution. More impressive than the long years of waiting for some complaints to be closed is the fact that some of them have negative, even extremely negative, results between their creation and the closing date. If the negative time was around -365 days, we could imagine a data entry problem, but this seems less likely with numbers like -66 days. It's the kind of problem we need to look at. We can identify another one, when we get a creation date range:

```R
> range(df$created_date)
[1] "2024-10-01 16:09:24 -03" "2025-01-26 03:33:15 -03"
```

Given the size of this CSV file (three months or so) and the fact that it must be constantly updated, it is a wonder that the first few lines are not from 2010, which should mark the first datasets. We expected the CSV to be continually appended. And even more surprising is the fact that the dates of 2024 are in the first lines. This suggests that we cannot determine the date ordering of the data in the file. We can visualize the date distribution from one line to the next by plotting a line graph on the index against the date:

![](https://github.com/DellaCortef/time-series-pocket-reference/blob/main/government-uses-of-time-series/images/cumulative-record-count-over-time.png?raw=true)

There is no way to avoid this problem. If we want to understand how behavior changes over time, we will have to deal with unordered data. But we have some options.

### Use Upsampling and aggregate Data as we iterate

One such option is to upsample the data as we iterate through it to build a condensed **time series** with aggregated statistics. We could choose a resolution and aggregation functions from the beginning of our analysis and then calculate them as we iterate. Then we could classify our results at the end of our analysis. It would be like having, say, a dictionary/list of all dates from 2024 (or earlier, 2010 for example) to the present and then simply adding the appropriate date to each line. This method would generate a list/dictionary with a relatively small number of entries and we could then sort them at the end based on date.

The advantage is that this would be relatively simple to code and would be a way to combine cleaning, exploration. data analysis in an exploratory stage. The disadvantage is that the detailed data would still be lost in the limbo of the unclassified file so that if there was a specific period of interest, we would have to search the entire unclassified file in order to find all the relevant entries.

### Classifying Data

Another option is to sort the data. It is a Herculean task, given the gigantic size of the file and the relatively disordered dates that we were able to observe. Even in the 2014 data, it doesn't appear that the dates are in order. Therefore, we have no indication that we can trust any data. In other words, we must consider this a pile of data in random order.

Sorting the entire file would require a lot of memory, but it might be worth it for two reasons. One of them is that we just classify once, and then we can save our results for any later analysis we want to do. The other is that we can have the full level of detail preserved. This way, if we identify specific time periods of interest in our analysis, we can examine the data in all its detail to understand what is happening. In concrete terms, and thinking about how we could do this, we have some options:

- Linux has a command line tool for sorting;
- most databases can classify data, so we can transfer it to a database and let the bank take care of this process;
- we could create our own sorting algorithm and implement it. It is necessary to formulate something that does not consume a huge amount of memory. The odds of our efforts matching what is available in pre-packaged rating options are slim.

We chose to use a Linux command line tool. Even though it takes some time to get this right, we can acquire new experiential skill as well as gain access to a correct and well-implemented classification for this large file. We start by creating a small test file to use:

```Linux
head -n 1000 311.csv | tail -n 999 > test.csv
```

Note the use of the *head* and *tail* commands. The *head* will include the first line of the file, which, for this file, gives the column names. If we use this command along with sorting the values, the column names will not be preserved as the top line of the file. So we cut them before sorting. If you are using a Linux-based operating system, you can use the *sort* command:

```Linux
sort --field-separator=',' --key=2,3 testsorted.csv
```

Here we identify the field separator and then indicate that we want to sort according to the second and third columns, that is, from the creation date and closing date (which we only know from a previous inspection of the file). We generate the output in a new file, as it wouldn't be very useful to have the standard output.

We can now inspect the sorted file in R, but unfortunately we will find that this does not return a sorted file either. We sort according to a date column (which would be processed as a string, not date-aware). However, the current date formatting starts with the month, so we will end up with a column of dates by month, rather than ordered by overall time, as we see when we review the "sorted" CSV resulting from our previous command.

As we can see, it makes sense to sort by string instead of using a sort by date. In fact, this signals one of the benefits of using proper ISO formatting for dates: you'll still get the correct sorting when sorting as a string, unlike the previous format. This is an example of a very common problem with "found" **time series** data: the available timestamp format may not be the most suitable for **time series** analysis.

We will take a more general approach to see if our available tools can handle this CSV if we only read certain columns. The first question we're interested in about this dataset is how the interval between creating a 311 claim and closing that claim may have varied over time. In this case, I suppose I need just two columns: 'created_date' and 'closed_date'. I will see if it is possible to read just two columns, a tiny portion of all the columns in terms of count and character count (because some columns are gigantic), in my simple notebook.

Now we can read all the rows of data and our further analysis will be on the entire data set, instead of just the first few rows:

Newbies to **Big Data** might be surprised by the fact that 19 million rows isn't that big, but that's how it is. In reality, a small portion of the data is enough to solve a relevant **time series** question. We soon noticed some shockingly incorrect numbers in the *LagTime* column - tens of thousands of days or negative days. We will eliminate these numbers and place a limit on the data, driven in part by a distribution of a random sample of data points:

We discard data with negative lag times, as we lack documentation or domain knowledge to know what they are. We also left out data that we considered to have extreme or unrealistic values, discarding data in which the lag time to close the complaint on line 311 was more than 1,000 days.

Now that we can store all the data of interest in memory at the same time, we can ask questions about the **time series** holistically. However, the question is whether and how the distribution of lag time may have changed over time. We could resort to a rolling or sliding window over the **time series**, but this is computationally heavy; we would have to do a lot of calculations repeatedly as we slid a window over the data. We also want to, as we could imagine the continuation of this project in the current data as it arrives. It would be best not to store this multigigabyte data file indefinitely.

## Online Statistical Analysis of Time Series data

We will use a very simple online quantile estimation tool, called *P-square algorithm* (https://perma.cc/G8LA-7738), but we will modify it so that it is time-aware. The original algorithm assumed that there was a stable distribution from which quantiles were being inferred, but we want to consider the case of distributions that change over time. As with an exponentially weighted moving average, we will add this time awareness by weighting past observations less. And we do this in the same way, introducing a factor to reduce the weights of previous measurements each time a new measurement becomes available.

The algorithm requires a little data recording, making it easier to implement in a more object-oriented programming language. Soon, we will switch from R to Python (I will leave the commands here in Markdown). However, note that we will use preprocessed data from R, as the **data.table** package has considerably better performance with this type of **Big Data** than the tools available in Python.

The version of the P-square algorithm that we will implement will generate a histogram of the values. So, as we create a *PQuantile* object, we allocate a predefined number of bins for our histogram calculations, bin positions, and ongoing observation calculations:

```Python
## Importing libs
import math
import bisect

sign = lambda x: (1, -1)[x < 0]

## Beginning of class definition
class PQuantile:
  def __init__(self, b, disocunt_factor):
    ## Initializing
    self.num_obs = 0
    ## Counts per quantile
    self.n = [i for i in range(self.b+1)]
    self.q = []
    
    ## b is the number of quantiles
    self.b = b
    ## The discount factor defines how we adjust previous counts when new data is available
    self.discount_factor = discount_factor
```

