{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1671dc82",
   "metadata": {},
   "source": [
    "In studies on machine learning and statistical analysis, the focus is predominantly on the performance of models in terms of accuracy. While accuracy should typically be the primary concern when evaluating a model, sometimes computational performance considerations are imperative when looking at large data sets or models that are widely deployed to serve large populations of client applications.\n",
    "\n",
    "**Time series** data sets become so large that you simply cannot perform any analysis - or cannot do them correctly - as they are quite demanding in terms of available computational resources. In these cases, many organizations do the following:\n",
    "\n",
    "- maximized computational resources (expensive and often wasteful, both economic and environmental);\n",
    "- conduct a project poorly (insufficient hyperparameter adjustment, insufficient data, etc...);\n",
    "- they do not create a project;\n",
    "\n",
    "Neither of these options is satisfactory, especially when you are just starting out with a new data set or new analytical technique. It can be frustrating not knowing whether your failures are the result of bad data, a thorny problem, or a lack of resources. Fortunately, we will cover some workarounds to expand your options for very demanding analyzes or huge data sets.\n",
    "\n",
    "The purpose of the notebook is to guide you with some considerations on how to reduce the computational resources required for training or inference on a specific model. Most of the time, these questions are specific to a particular data set, as well as the resources you have available and your accuracy and speed goals. In this chapter, we will address these concerns, with the hope that they partially cover the problems you encounter and can inspire future brainstorming. These considerations will come to the fore when you have completed your first few rounds of analysis and modeling and should not be a priority when you are dealing with a problem for the first time. However, when the time comes to put something into production or extend a small research project, you should revisit these concerns frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8638799c",
   "metadata": {},
   "source": [
    "## Working with Tools Built for General Use Cases\n",
    "\n",
    "One challenge with time series data is that most tools, particularly those for machine learning, are built for a more general use case, and most illustrative examples show the use of cross-sectional data. But these machine learning methods are not as efficient with **time series** data. The solutions to your individual problems will vary, but the general ideas are the same. \n",
    "\n",
    "### Models Built for Cross-Sectional Data Do Not \"Share\" Data Across Samples\n",
    "\n",
    "In many cases, when feeding discrete samples of **time series** data to an algorithm, most often machine learning models, you will notice that large chunks of data being fed between the samples overlap. For example, suppose you have the following data on monthly widget sales:\n",
    "\n",
    "\n",
    "| Month    | Sold Widgets |\n",
    "|----------|------------:|\n",
    "| Jan 2014 | 11,221 |\n",
    "| Feb 2014 |  9,880 |\n",
    "| Mar 2014 | 14,423 |\n",
    "| Apr 2014 | 16,720 |\n",
    "| May 2014 | 17,347 |\n",
    "| Jun 2014 | 22,020 |\n",
    "| Jul 2014 | 21,340 |\n",
    "| Aug 2014 | 25,973 |\n",
    "| Sep 2014 | 11,210 |\n",
    "| Oct 2014 | 11,583 |\n",
    "| Nov 2014 | 11,539 |\n",
    "| Dec 2014 | 10,240 |\n",
    "\n",
    "You are trying to make predictions by mapping each \"shape\" to a nearest neighboring curve. You prepare many formats from this data. Here, we list just a few of these data points, as you may want to use six-month curves as the \"shapes\" of interest (note that we are not doing any data preprocessing to normalize or raw additional features of interest, such as moving averages or smoothed curves). ).\n",
    "\n",
    "| Col 1 | Col 2 | Col 3 | Col 4 | Col 5 | Col 6 |\n",
    "|-------|-------|-------|-------|-------|-------|\n",
    "| 11221 | 9880  | 14423 | 16720 | 17347 | 22020 |\n",
    "|  9880 | 14423 | 16720 | 17347 | 22020 | 21340 |\n",
    "| 14423 | 16720 | 17347 | 22020 | 21340 | 25973 |\n",
    "\n",
    "\n",
    "Interestingly, all we were able to do with this input preparation was make our dataset six times larger without including any additional information. And from a performance point of view, this is a real catastrophe, even though it is necessary for the inputs of a variety of machine learning modules. If you have encountered this problem, consider some solutions.\n",
    "\n",
    "#### Do not use overlapping data\n",
    "\n",
    "Think about generating just one \"data point\" so that each month weaves its way along just one curve. If you do this, the previous data might look similar to the following table:\n",
    "\n",
    "| Col 1 | Col 2 | Col 3 | Col 4 | Col 5 | Col 6 |\n",
    "|-------|-------|-------|-------|-------|-------|\n",
    "| 11221 | 9880  | 14423 | 16720 | 17347 | 22020 |\n",
    "| 21340 | 25973 | 11210 | 11583 | 12014 | 11400 |\n",
    "\n",
    "Note that this would be quite easy, because it amounts to simple array reshaping, rather than custom data repetition.\n",
    "\n",
    "#### Use a generator-type paradigm to iterate through the dataset\n",
    "\n",
    "Using a generator-like paradigm to iterate through the dataset, resampling the same data structure as appropriate, is easy in Python, but you can also use R and other languages. If we imagine that the original data is stored in a 1D **NumPy** array, this would look like the following code (note that this would have to be coupled with a machine learning data structure or algorithm that accepts generators):\n",
    "\n",
    "```python\n",
    "def array_to_ts(arr):\n",
    "    ids = 0\n",
    "    while idx + 6 <= arr.shape[0]:\n",
    "        yield arr[idx:(idx+6)]\n",
    "```\n",
    "\n",
    "Note that it is recommended to program data modeling code that does not unnecessarily harm a data set, both from a training and production point of view. In training, this will enable you to fit more training examples into memory, and in production, you will be able to perform multiple predictions with fewer training resources, in the case of predictions (or classifications) on overlapping data. If you are making frequent predictions for the same iso case, you are probably working with overlapping data; Therefore, this problem and its solutions will be very relevant.\n",
    "\n",
    "### Models that are not Pre-Calculated generate unnecessary Lag between Measuring Data and Making a Prediction\n",
    "\n",
    "Typically, machine learning models do not prepare for or take into account the possibility of pre-calculating part of a result before having all the data. However, this is a very common scenario for **time series**.\n",
    "\n",
    "If you are making your model available in a time-sensitive application, such as for medical predictions, vehicle location estimates, or stock price forecasting, you may find that the lag of calculating a forecast only after all the data is available is enormous. In this case, consider whether the chosen model can be partially pre-calculated in advance. Let’s look at some examples of how this is possible:\n",
    "\n",
    "- if you are using a recurrent neural network that takes multiple channels of information in 100 different time steps, you can pre-calculate/unroll the neural network in the first 99 time steps. So when the last data point finally arrives, you only need to do one final set of matrix multiplications (and other activation function calculations) instead of 100. In theory, this speeds up your response time by 100 times.\n",
    "- se estiver usando um modelo AR(5), voc6e pode pré-calcular tudo, exceto o termo mais recente na soma que constitui o modo. Vale lembrar que um processo AR(5) se parece com a equação a seguir. Se está prestes a gerar um previsão, você já conhece os valores de *y<sub>t - 4</sub>*, *y<sub>t - 3</sub>*, *y<sub>t - 2</sub>*, e *y<sub>t - 1</sub>*, o que significa que você pode ter tudo, exceto o *phi<sub>0</sub> Ã y<sub>t</sub>* pronto para usar antes de saber *y<sub>t</sub>*:\n",
    "\n",
    "*y<sub>t + 1</sub> = phi<sub>4</sub> × y<sub>t - 4</sub> + phi<sub>3</sub> × y<sub>t - 3</sub> + phi<sub>2</sub> × y<sub>t - 2</sub> + phi<sub>1</sub> × y<sub>t - 1</sub> + phi<sub>0</sub> × y<sub>t</sub>*\n",
    "\n",
    "- if you are using a clustering model to find the nearest neighbors, synthesizing the characteristics of a **time series** (mean, standard deviation, maximum, minimum, etc.), you can calculate these characteristics with a **time series** with one less data point and run your model with that **time series** to identify multiple nearest neighbors. You can then update these characteristics once the final value is reached and run the entire analysis again with only the closest neighbors found in the first round of analysis. This will actually require more switching resources, but will result in a shorter time lag between the final measurement and forecast delivery.\n",
    "\n",
    "In many cases, your model may not perform as slowly due to network lag or other factors, so pre-calculation is a worthwhile technique only when feedback timing is extremely important and when you are confident that the Model calculation is contributing to the time between an application receiving all the necessary information and outputting a useful prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a1a4d1",
   "metadata": {},
   "source": [
    "## Data Storage Formats: Advantages and Disadvantages\n",
    "\n",
    "An overlooked area when it comes to performance bottlenecks for training and productizing **time series** models is data storage. Let's look at some common mistakes:\n",
    "\n",
    "- *data storage in a row-based data format, even though the **time series** is formed by traversing a column*. This results in data where adjacent points in time are not adjacent in memory;\n",
    "- *storage of raw data and execution of analyzes based on this data*. Depending on the model, it is preferable to have preprocessed data that has been downsampled, as far as possible.\n",
    "\n",
    "Next, we'll look at these data storage factors so that your model training and inference happens as quickly as possible.\n",
    "\n",
    "### Store your Data in Binary Format\n",
    "\n",
    "It's tempting to store data in a comma-separated text file, like a CSV file. Normally, this is how the data is provided, so inertia leads us to make this choice. These file formats are also human-readable, which makes it easier to check the data in the file against the pipeline outputs. Lastly, this data is generally easy to upload to different platforms.\n",
    "\n",
    "However, it is not easy for your computer to read text files. If you are working on data sets so large that you cannot fit all of your data into memory during training, you will be dealing with I/O and related processing associated with the file format you choose. By storing data in a binary format, you can substantially reduce I/O-related slowdowns in several ways:\n",
    "\n",
    "- as the data is in binary format, your data processing package already \"understands\" it. There is no need to read a CSV and transform them into a data frame. When inputting the data, you will have a data frame;\n",
    "- since the data is in binary format, it can be compressed better than a CSV or other text-based file. That is, the I/O itself will be shorter, as there is less physical memory to read from a file and recreate its contents.\n",
    "\n",
    "Binary storage formats are easily accessible in R and Python. In R, use *save()* and *load()* for **data.table**. In Python, use *pickling* and note that both **Pandas** (pd.DataFrame.load(), pd.DataFrame.save()) and **NumPy** (np.load() and np .save()) include wrappers around pickling that you can use for your specific objects.\n",
    "\n",
    "### Preprocess your Data so that You can \"Swipe\" over it\n",
    "\n",
    "This recommendation is related to \"Models Built for Cross-Sectional Data Do Not \"Share\" Data Across Samples\". In this case, you should also think about how to preprocess your data and ensure that the way you do this is consistent with using a sliding window over that data to generate multiple test samples.\n",
    "\n",
    "As an example, consider normalization or moving averages as preprocessing steps. If you plan to do this for each time window, you could get better model accuracy (although, in my experience, these gains are often intimate). However, there are several disadvantages:\n",
    "\n",
    "- more computational resources are needed to calculate these preprocessing features over and over again on overlapping data - only to end up with very similar numbers;\n",
    "- you need to store overlapping data with slightly different preprocessing over and over again;\n",
    "- you can't get the most out of it when you slide a window over your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b19e557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
