{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0460f316",
   "metadata": {},
   "source": [
    "In studies on machine learning and statistical analysis, the focus is predominantly on the performance of models in terms of accuracy. While accuracy should typically be the primary concern when evaluating a model, sometimes computational performance considerations are imperative when looking at large data sets or models that are widely deployed to serve large populations of client applications.\n",
    "\n",
    "**Time series** data sets become so large that you simply cannot perform any analysis - or cannot do them correctly - as they are quite demanding in terms of available computational resources. In these cases, many organizations do the following:\n",
    "\n",
    "- maximized computational resources (expensive and often wasteful, both economic and environmental);\n",
    "- conduct a project poorly (insufficient hyperparameter adjustment, insufficient data, etc...);\n",
    "- they do not create a project;\n",
    "\n",
    "Neither of these options is satisfactory, especially when you are just starting out with a new data set or new analytical technique. It can be frustrating not knowing whether your failures are the result of bad data, a thorny problem, or a lack of resources. Fortunately, we will cover some workarounds to expand your options for very demanding analyzes or huge data sets.\n",
    "\n",
    "The purpose of the notebook is to guide you with some considerations on how to reduce the computational resources required for training or inference on a specific model. Most of the time, these questions are specific to a particular data set, as well as the resources you have available and your accuracy and speed goals. In this chapter, we will address these concerns, with the hope that they partially cover the problems you encounter and can inspire future brainstorming. These considerations will come to the fore when you have completed your first few rounds of analysis and modeling and should not be a priority when you are dealing with a problem for the first time. However, when the time comes to put something into production or extend a small research project, you should revisit these concerns frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47905935",
   "metadata": {},
   "source": [
    "## Working with Tools Built for General Use Cases\n",
    "\n",
    "One challenge with time series data is that most tools, particularly those for machine learning, are built for a more general use case, and most illustrative examples show the use of cross-sectional data. But these machine learning methods are not as efficient with **time series** data. The solutions to your individual problems will vary, but the general ideas are the same. \n",
    "\n",
    "### Models Built for Cross-Sectional Data Do Not \"Share\" Data Across Samples\n",
    "\n",
    "In many cases, when feeding discrete samples of **time series** data to an algorithm, most often machine learning models, you will notice that large chunks of data being fed between the samples overlap. For example, suppose you have the following data on monthly widget sales:\n",
    "\n",
    "\n",
    "| Month    | Sold Widgets |\n",
    "|----------|------------:|\n",
    "| Jan 2014 | 11,221 |\n",
    "| Feb 2014 |  9,880 |\n",
    "| Mar 2014 | 14,423 |\n",
    "| Apr 2014 | 16,720 |\n",
    "| May 2014 | 17,347 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7e8796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
