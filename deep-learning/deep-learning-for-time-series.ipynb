{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f657939",
   "metadata": {},
   "source": [
    "# Deep Learning for Time Series\n",
    "\n",
    "Deep learning for **time series** is a relatively new but promising initiative. As it is a very flexible technique, it can be advantageous for analyzing **time series**. In addition to being promising, the technique makes it possible to model extremely complex and non-linear temporal behaviors without the need to assume their functional formats - which could be a game changer for non-statistical forecasting techniques.\n",
    "\n",
    "Deep learning represents a subarea of ​​machine learning in which a \"graph\" is constructed to connect input nodes into a complicated structure of nodes and edges. When moving from one node to another via an edge, a value is multiplied by the weight of that edge and then typically passes through some type of non-linear activation function. It's this nonlinear activation function that makes deep learning so interesting: it allows us to fit extremely complex nonlinear data, something that hasn't been done so successfully before.\n",
    "\n",
    "Deep learning has mostly taken hold over the last ten years, as improvements in commercially available hardware have been combined with vast amounts of data to make this kind of heavy model tuning possible. Deep learning models can have millions of parameters. So one way to understand them is to imagine any graph with all kinds of matrix multiplications and nonlinear transformations, and then imagine a smart optimizer that optimizes its model on a small group of data at a time, continually adjusting the weights of that data. large model so that they generate increasingly better results.\n",
    "\n",
    "Deep learning has not yet generated surprising prediction results as it has in other areas, such as image processing and natural language processing. Despite this, there is good reason to be optimistic that deep learning will ultimately advance the art of prediction while minimizing the sensitive and highly uniform nature of the assumptions and technical requirements common to traditional prediction models.\n",
    "\n",
    "Most of the hurdles of data preprocessing to adjust a model's assumptions disappear when using deep learning models:\n",
    "- stationarity is not required;\n",
    "- the need to develop the art and skill of selecting parameters, such as evaluating the seasonality and order of a seasonal ARIMA model, disappears;\n",
    "- the need to develop a hypothesis about the underlying dynamics of a system, which often helps in state space modeling, disappears.\n",
    "\n",
    "Deep learning is even more flexible for several reasons:\n",
    "- many machine learning algorithms tend to be quite sensitive in terms of dimensionality and types of input data needed for the training algorithm to work. In contrast, deep learning is quite flexible regarding the model and nature of the inputs;\n",
    "- heterogeneous data can be challenging with many commonly used machine learning techniques, despite being commonly used with deep learning models;\n",
    "- while deep learning has a lot of flexibility when it comes to developing specific architectures for temporal data, machine learning models are rarely developed for **time series** problems.\n",
    "\n",
    "However, deep learning is not a cure-all. While stationarity is not a requirement for deep learning applied to **time series**, in practice deep learning does not fit data well with a trend unless standard architectures are modified to fit the trend . That is, we will still need to preprocess our data or our technique.\n",
    "\n",
    "What's more, deep learning does best with numeric inputs in different channels, all scaled to similar values between -1 and 1. This means you'll need to preprocess your data, although theoretically it's not necessary. You'll also need to do some preprocessing to avoid lookahead, something the deep learning community generally doesn't spend much time perfecting.\n",
    "\n",
    "Finally, deep learning optimization and modeling techniques for time-oriented neural networks (the largest class of which are recurrent neural networks or RNNs) are not as well developed as those for image processing (the largest class of which are convulsional neural networks, or CNNs). That is, you will find less guidance on best practices and general rules for selecting and training an architecture than you would for non-temporal tasks.\n",
    "\n",
    "In addition to these difficulties of applying deep learning to **time series**, you will discover the advantages of doing so have their positive and negative sides. For starters, deep learning performance for **time series** does not invariably outperform more traditional **time series** accuracy and classification methods. In fact, prediction is a ripe area for deep learning improvements.\n",
    "\n",
    "However, there are reasons to expect immediate and long-term benefits from adding deep rendering to your **time series** analysis toolset. First, large technology companies began to promote deep learning for **time series** services with custom architectures they developed in-house, often considering specific modeling tasks. You can use these services or combine them with your own analytics to get good performance.\n",
    "\n",
    "Second, you might have a dataset that works really well with deep learning for **time series**. In general, the stronger the signal-to-noise ratio, the better its performance. Next, we will briefly review the concepts that inspired and supported deep learning as a mathematical and computer science pursuit, as well as concrete examples of code to apply to deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d4de93",
   "metadata": {},
   "source": [
    "## Deep Learning Concepts\n",
    "\n",
    "Deep learning has roots in countless fields. Inspiration was sought in biological sciences, and computer scientists, as well as quantitative analysts, kept questioning whether the way to build intelligent machines was, after all, to simulate a brain with its neural networks that triggered responses to certain triggers. Inspiration was sought in mathematics through the Universal Approximation Theorem, evidence of which dates back to the late 1980s and early 1990s. Finally, there was the advancement of computational resources and availability, along with the growing area of ​​machine learning whose Success showed that, with enough data and parameters, complicated systems could be modeled and predicted. Deep learning took all of these ideas even further, creating networks with millions of parameters, trained on large data sets and with the theoretical basis that a neural network should be able to represent an arbitrary and non-linear function with a high degree of accuracy. The figure below shows a simple neural network, a multilayer *perceptron* (or fully connected network).\n",
    "\n",
    "In the figure below, we can see how multichannel inputs are provided to the model in the form of a vector of dimension *d*. Nodes represent input values, while edges represent multipliers. All edges entering a node represent a previous value multiplied by the value of the edge it crossed. The values ​​of all inputs in a single node are summed and then passed to a non-linear activation function, creating a non-linearity.\n",
    "\n",
    "We can see that the input consists of two channels, or a vector of length 2. And we have 4 hidden units. We multiply each of the 2 inputs by a different weight for each of the 4 hidden units for which it is intended, that is, we need 2 x 4 = 8 weights to completely represent the problem. Furthermore, since we will combine the results of these various multiplications, matrix multiplication is not just *analogous* to what we are doing, but *precisely* what we are doing.\n",
    "\n",
    "If we documented the steps of what we are doing, it would look like this:\n",
    "\n",
    "1. The input vector i has two elements. The layer 1 weights are denoted by W<sub>1</sub>, a 2 × 4 matrix, so we calculate the hidden layer values as W<sub>1</sub> × i<sub>1</sub>. This results in a 4 × 1 matrix, despite not actually being the output of the hidden layer:\n",
    "   \n",
    "    - W<sub>1</sub> × i<sub>1</sub>\n",
    "    \n",
    "    \n",
    "2. It is necessary to apply a nonlinearity, which we can do with several \"activation functions\", such as the hyperbolic tangent (tanh) or the sigmoid function (σ). Typically we will also apply a bias, B1, within the activation function, so that the hidden layer output is actually:\n",
    "    \n",
    "    - H = a(W<sub>1</sub> × X<sub>1</sub> + B<sub>1</sub>)\n",
    "    \n",
    "    \n",
    "3. In the neural network represented below, we have two outputs to predict. For this reason, we need to convert the four-dimensional hidden state output into two outputs. Traditionally, the last layer does not include a non-linear activation function, unless our calculation applies a *softmax* activation function in the case of a categorization. Let's assume we're just trying to predict two numbers, not two probabilities or categories. So, we just need to apply one last \"dense layer\" to combine the four outputs of the hidden layer per final output. This dense layer will combine four inputs into two outputs, so we will need a 2 × 4 matrix, W<sub>2</sub>:\n",
    "    \n",
    "    - Y = W<sub>2</sub> × H\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "I hope this gave you an idea of ​​how these models work. The general idea is to have lots of parameters and opportunities for nonlinearities. Choosing the appropriate number and format of parameters and hyperparameters is something of an art, as well as being an accessible problem. The important thing is to learn how to train these parameters, initialize them intelligently from the beginning, and make sure the model is headed in the right direction toward a reasonably good solution. As this is a non-convex class of models, the intention is not to find the global ideal. Instead, as the reasoning dictates, as long as we find intelligent ways to regularize the model, we should just identify a local ideal that is \"good enough\" to meet the needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5764693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the image\n",
    "display(Image(filename=\"/Users/dellacorte/py-projects/data-science/time-series-pocket-reference/deep-learning/images/feedforward-neural-network.jpeg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53e1cd6",
   "metadata": {},
   "source": [
    "## Programming a Neural Network\n",
    "\n",
    "#### Symbolic and Imperative Programming Styles\n",
    "\n",
    "We have flexibility in deep learning programming to use a *symbolic* or *imperative* programming style. In *symbolic* style, we declare all relationships in advance without them being computed at the time of declaration. On the other hand, in the *imperative* style, the calculation occurs when it is programmed, line by line, without waiting to account for what will come later. Each style has its advantages and disadvantages, and they are not inflexible:\n",
    "- *symbolic* programming is usually more efficient because you leave room for the structure to optimize calculations instead of executing them when and how they are ordered;\n",
    "- *imperative* programming is usually easier to debug and understand;\n",
    "- **TensorFlow** and **MXNet** tend to have a *symbolic* programming style, while **Torch** is more *imperative*\n",
    "\n",
    "\n",
    "### Data, Symbols, Operations, Layers and Graphs\n",
    "\n",
    "In general, deep learning frameworks focus on the notion of a graph and its construction. The idea is that any architecture can be represented in terms of its individual components and their relationship to each other. Furthermore, the idea of variables dissociated from real values ​​is also essential. In other words, we can have a symbol A, a symbol B and a third symbol C, the result of the multiplication of the matrix A and B:\n",
    "\n",
    "```python\n",
    "symbolA;\n",
    "symbol B;\n",
    "symbol C = matmul(A, B);\n",
    "```\n",
    "\n",
    "The distinction between symbols and data is essential for each framework due to the relationship between the symbol and the data. The symbol is used to learn a more general relationship; the data may be noisy. There is only one symbol A, even though we have millions or billions of values to supply A, each combined with its respective B and C.\n",
    "\n",
    "When we take a step back and think about what to do with this data, we come across some operations. We can add or multiply symbols together and we can think of them as operations. We can also perform univariate operations, such as changing the shape of a symbol (perhaps converting symbol A from a 2 × 4 matrix to an 8 × 1 matrix) or passing values ​​to an activation function, such as calculating tanh(A). If we go back even further, we can think of layers as traditional processing units that we associate with common architectures, like a fully connected layer seen in the previous section. Taking into account the activation function and bias, as well as the main operation of the matrix, we can say:\n",
    "\n",
    "- layer L = tanh(A × B + bias)\n",
    "\n",
    "In many frameworks, this layer may be expressed as one or two layers rather than several operations, depending on whether a combination of operations is popular enough to warrant its own assignment as a unit of a layer.\n",
    "\n",
    "Finally, we can relate several layers to each other, passing one to the next:\n",
    "\n",
    "- layer L1 = tanh(A × B + bias1)\n",
    "- layer L2 = tanh(L1 × D + bias2)\n",
    "\n",
    "The cluster of all these symbols, operations and layers results in a graph. A graph does not need to be fully connected - that is, all symbols will not necessarily depend on each other. A graph is used precisely to classify which symbols depend on other symbols and how. This is essential so that when calculating gradients descent we can adjust our weights at each iteration to get a better solution. The good thing is that most modern deep learning packages take care of all these calculations. No need to specify what depends on what and how the gradient changes with each added layer - everything is inside the package.\n",
    "\n",
    "And most importantly, as mentioned before, we can use high-level APIs so that we won't need to describe matrix multiplications. If we want to use a fully connected layer, the equivalent of a matrix multiplication, followed by a matrix addition, and then an element-wise activation function of some kind, we don't need to write all these calculations. for example, at **mxnet**, we can do this with the following code:\n",
    "\n",
    "```python\n",
    "import mxnet as mx\n",
    "fc1 = mx.gluon.nn.Dense(120, activation='relu')\n",
    "```\n",
    "\n",
    "This will give us a fully connected layer that converts the inputs to an output dimension of 120. Furthermore, this line represents everything we just mentioned: a matrix multiplication, followed by a matrix addition, followed by an element-wise activation function.\n",
    "\n",
    "Of course, this simple code doesn't cover everything we need, not even for a short deep learning task. You need to describe your inputs, your targets and how you want to calculate the loss. You also need to place your layer within a model. We will do this in the code below importing the libs.\n",
    "\n",
    "#### Popular Frameworks\n",
    "\n",
    "As in many technology domains, the offering landscape in the world of deep learning is constantly changing. And, just like in the business world, different mergers and acquisitions often occur, such as the merger of **TensorFlow** and **Theano**, and **MXNet** and **Caffe**. No framework has established itself as a leader in **time series** deep learning, although **MXNet** may be the best bet for the near future, given Amazon's own interest in forecasting and its recently launched deep learning for **time series** forecasting services. For someone new to the field, the most important and widely used libraries, and their respective technological giants, are:\n",
    "\n",
    "- **TensorFlow** (Google)\n",
    "- **MXNet** (Amazon)\n",
    "- **Torch** (Facebook)\n",
    "\n",
    "However, there are often academic and opensource start-up libraries that offer innovations that can dominate an area or at least make a substantial contribution to an existing framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03110425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mxnet\n",
      "  Downloading mxnet-1.9.1-py3-none-macosx_10_13_x86_64.whl (39.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.5/39.5 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.20.0 in /Users/dellacorte/opt/anaconda3/lib/python3.9/site-packages (from mxnet) (2.28.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /Users/dellacorte/opt/anaconda3/lib/python3.9/site-packages (from mxnet) (1.21.6)\n",
      "Collecting graphviz<0.9.0,>=0.8.1\n",
      "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/dellacorte/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.20.0->mxnet) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dellacorte/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.20.0->mxnet) (2024.7.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/dellacorte/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.20.0->mxnet) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dellacorte/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.20.0->mxnet) (3.3)\n",
      "Installing collected packages: graphviz, mxnet\n",
      "  Attempting uninstall: graphviz\n",
      "    Found existing installation: graphviz 0.20.1\n",
      "    Uninstalling graphviz-0.20.1:\n",
      "      Successfully uninstalled graphviz-0.20.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dtreeviz 2.2.1 requires graphviz>=0.9, but you have graphviz 0.8.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed graphviz-0.8.4 mxnet-1.9.1\n"
     ]
    }
   ],
   "source": [
    "# Install libs\n",
    "!pip install mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7120bd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libs\n",
    "import os\n",
    "import mxnet as mx\n",
    "from mxnet.gluon import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from mxnet import gluon, init, autograd\n",
    "from IPython.display import Image, display\n",
    "from mxnet.gluon.data import DataLoader, ArrayDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78288798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data (replace with your real data)\n",
    "X = mx.nd.random.uniform(shape=(100, 2))\n",
    "y = mx.nd.random.uniform(shape=(100, 1))\n",
    "\n",
    "# Create a DataLoader to load data in batches\n",
    "batch_size = 16\n",
    "dataset = ArrayDataset(X, y)\n",
    "train_data = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the neural network\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(120, activation='relu'),\n",
    "        nn.Dense(1))\n",
    "net.initialize(init=init.Xavier())\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "L2Loss = gluon.loss.L2Loss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.01})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bc3bfb",
   "metadata": {},
   "source": [
    "Lastly, assuming we also configure our data as needed, we can step through a training epoch (i.e., one pass through all the data) like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24ed86a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 0.18967264890670776\n",
      "Epoch 2: Loss = 0.10500496625900269\n",
      "Epoch 3: Loss = 0.056160204112529755\n",
      "Epoch 4: Loss = 0.09042498469352722\n",
      "Epoch 5: Loss = 0.044622767716646194\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in range(5):\n",
    "    for data, target in train_data:\n",
    "        # Calculate the gradient\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = L2Loss(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        trainer.step(batch_size)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}: Loss = {loss.mean().asscalar()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b5e8f8",
   "metadata": {},
   "source": [
    "In most packages, it is also easy, as with **mxnet**, to save your model and the parameters that accompanied it for later use in production or further training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8236ad07",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.save_parameters('model.params')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5c021c",
   "metadata": {},
   "source": [
    "In the following examples, we will use **mxnet**'s **Module API** to exemplify another style of creating graphs and training models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139bb707",
   "metadata": {},
   "source": [
    "## Building a Training pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3227fd32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
