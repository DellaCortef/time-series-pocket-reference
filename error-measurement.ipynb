{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d289f487",
   "metadata": {},
   "source": [
    "# Error Measurement\n",
    "\n",
    "So far, we have used a variety of measures to compare models or judge how well a model performed its task. Now, we will analyze best practices for judging the accuracy of forecasts, emphasizing the specific issues regarding **time series** data.\n",
    "\n",
    "For those new to time series forecasting, it is most important to understand that standard cross-validation is typically not recommended. It is not possible to select randomly sampled training, validation, and testing data sets for each of these categories in a time-independent manner.\n",
    "\n",
    "However, things are even more complicated. You need to think about how different data samples relate to each other over time, even though they appear independent. For example, suppose you are working on a **time series** classification task, so that you have many separate **time series** samples, each of which is its own data point. It may be tempting to think that in this case it is possible to randomly choose **time series** for each training, validation and test set, but this does not work. The problem with this approach is that it does not reflect how you would use your model, i.e. it would not reflect training your model on earlier data nor testing it on later data.\n",
    "\n",
    "We don't want future information to leak into your model, as modeling doesn't work like that in practice. In turn, this means that the prediction error we measure in our model will be lower during testing than in production, since in testing we will have used cross-validation in our model in order to generate future information.\n",
    "\n",
    "Let's look at a realistic scenario of how this could happen. Imagine you are training an air quality detector for major cities in the Western US. In your training set, you include all data from 2017 and 2018 for San Francisco, Salt Lake City, Denver, and San Diego. And your test suite, you include the same date range for Las Vegas, Los Angeles, Oakland, and Phoenix. You discover that your air quality model does very well in Las Vegas and Los Angeles measurements, but it does even better in 2018. Great.\n",
    "\n",
    "Then you try to replicate the model training process on data from previous decades and find that it doesn't perform as well in the test as it does in the training run. So you remember the record-breaking wildfires in Southern California in 2018 and realize that they were \"incorporated\" into the original test/training because your training set gave you a window into the future. This is precisely why we should avoid standard cross-validation.\n",
    "\n",
    "There are times when propagating information from the future to choosing a model is not a problem. For example, if you are just trying to understand the dynamics of a **time series** when testing the quality level of a forecast, you are not trying to make a prediction, but rather testing the best possible fit of a given model to the data. In this case, including future data helps you understand the dynamics, although you should be careful about overfitting. And even in this case, there is no doubt that maintaining a valid test set - whose requirement is not to allow information to leak in the future - would still justify concerns about **time series** and cross-validation.\n",
    "\n",
    "Now that we've clarified things, let's go back to a concrete example of splitting data for training, validating, and testing a model. Next, we'll look more generally at how to determine when a prediction is good enough, or as good as possible. We will also examine how to estimate the uncertainty of our forecast when using techniques that do not directly produce an uncertainty or error measure as part of the output. We will end the chapter with a list of pitfalls that can help with building your **time series** model or preparing to put it into production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3c732c",
   "metadata": {},
   "source": [
    "## Basic Concepts: How to Test Predictions\n",
    "\n",
    "The most important element is to ensure that you are only building with data that can be accessed far enough in advance and can be used to generate prediction. For this reason, you need to think not only about when events happen, but also when the data will be available to you.\n",
    "\n",
    "While this sounds simple, remember that common preprocessing such as exponential smoothing can accidentally leak from the training period into the testing period. You can test this by first fitting a linear regression to an autoregressive **time series** and then to an exponentially smoothed autoregressive **time series**. You will notice that the more you smooth the **time series**, and the longer the smoothing half-life, the \"better\" your predictions become. This is because you are actually having to make less and less of a prediction, as more and more of your value is made up of an exponential average of previous values. In other words, it is a dangerous and treacherous lookahead that, despite this, still appears in academic articles.\n",
    "\n",
    "Bearing in mind these dangers and other hard-to-perceive ways of feeding the past into the future and vice versa, the gold standard for any model should be backtesting with training, validation and roll-forward testing.\n",
    "\n",
    "In backtesting, a model is developed for a set or ranges of data and then extensively tested on historical data, ideally representing the full range of possible conditions and variability. It is also important to emphasize that professionals need well-founded reasons to backtest a specific model and should avoid testing too many models. As most data analysts know, the more models you test, the more likely that model will overfit the data - that is, the more likely it will choose models with overly specific details about the current data set. rather than generalizing it robustly. Unfortunately, for **time series** professionals, this means a tricky balancing act that can lead to embarrassing results when putting models into production.\n",
    "\n",
    "But how do we implement backtesting? We do this implementation in a way that preserves a structure similar to cross-validation while being temporally aware. The common paradigm, assuming you have data representing the sequential passage of time \"in alphabetical order\", is as follows:\n",
    "\n",
    "        Train[A]               test with[B]\n",
    "        Train[A B]             test with[C]\n",
    "        Train[A B C]           test with[D]\n",
    "        Train[A B C D]         test with[E]\n",
    "        Train[A B C D E]       test with[E]\n",
    "<br>\n",
    "\n",
    "![The standard of excellence for evaluating the performance of a **time series** model, roll-forward training, validation and testing window](https://analisemacro.com.br/wp-content/uploads/2022/05/tscv.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "You can also move the training data window instead of expanding it. In this case, your training could look something like this:\n",
    "\n",
    "        Train[A B]         test with[C]\n",
    "        Train[B C]         test with[D]\n",
    "        Train[C D]         test with[E]\n",
    "        Train[D E]         test with[F]\n",
    "        \n",
    "<br>\n",
    "\n",
    "The method you choose depends in part on whether you think the behavior of your series is evolving over time. If so, it's best to use a rolling window so that every test period is tested with a model trained on the most relevant data. You may want to avoid overfitting, in which case using an expanding window will discipline your model better than a fixed-length window. Since this type of continuous division is a common training need, R and Python can easily generate them:\n",
    "\n",
    "- in Python, an easy way to generate data splits is through *sklearn.model_selection.TimeSeriesSplit*\n",
    "- in R, *tsCV* from the **fprecast** package will advance a model in time using backtesting and report errors\n",
    "\n",
    "There are other packages in R and Python that will do the same. You can also write your own functions to split your data, if you have ideas about how to implement this model test in a specific project. Maybe you want to ignore certain time periods because they exhibited anomalous dynamics, or maybe you want to weight the performance of certain time periods more.\n",
    "\n",
    "For example, suppose you work with financial data. Depending on your goals, it may be worth excluding data from extraordinary periods, such as the 2008 financial crisis. Or, if you work with retail data, you may want to consider model performance more for the Christmas shopping season, even if it sacrifices some of the accuracy in forecasting low-volume seasons.\n",
    "\n",
    "\n",
    "### Model-Specific Considerations for Backtesting\n",
    "\n",
    "Consider the dynamics of the model you are training when structuring your backtesting, especially when training a model with a certain time range of data. With traditional statistical models such as ARIMA, all data points are factored equally when selecting model parameters. Therefore, the more data, the lower the accuracy of the model, if you think that the model parameters should vary over time. This is also true for machine learning models where all training data is factored equally.\n",
    "\n",
    "On the other hand, stochastic batch methods can result in weights and estimates that evolve over time. Thus, if you train the data in chronological order, neural network models trained with typical stochastic gradient descent methods will, to some extent, consider the temporal nature of the data. The most recent gradient adjustments to the weight will depict the most recent data. In most cases, **time series** neural network models are trained on data in chronological order, so they tend to generate better results than models trained on data in random order.\n",
    "\n",
    "State space models also provide opportunities for tuning to adapt over time with mode. This contributes to a longer training window, because a long time window will not prevent the subsequent estimate from evolving over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6539438e",
   "metadata": {},
   "source": [
    "## When Is Your Forecast Good Enough?\n",
    "\n",
    "The quality of your forecast will depend on your overall objectives, the minimum quality level required for what you need to do, and the limits and nature of your data. If your data has a very high signal-to-noise ratio, you should have expectations for your model.\n",
    "\n",
    "Remember: a **time series** model is not perfect. But you should aim to do as well as or a little better than alternative methods, such as solving a system of differential equations about climate change, asking a knowledgeable stock broker for a tip, or turning to a medical textbook that shows you how. classify an EEG. When evaluating performance, keep in mind the known domain-specific limits on prediction as indicated by measurements - for now the upper bound on performance in many prediction problems.\n",
    "\n",
    "There are times when you know the model you are walking isn't good enough and you can do better. Let’s look at some things we can do to identify these opportunities:\n",
    "\n",
    "*Plot the model outputs for the test set*\n",
    "\n",
    "    - the distribution generated by the model should match the distribution of the values ​​you are trying to predict, assuming there is no expected regime shift or underlying trend. For example, if you are trying to predict stock prices and knowing that these prices fall and rise with the same frequency, if the model always predicts a rise, you have an inadequate model. Sometimes the distributions will be clearly wrong, while other times we can apply a statistical test to compare your model output to your actual targets.\n",
    "    \n",
    "*Plot model residuals over time*\n",
    "\n",
    "    - if the residuals are not homogeneous over time, your model was not specified. The temporal behavior of the residuals may indicate additional parameters needed in the model to represent the temporal behavior.\n",
    "    \n",
    "*Test the model against a simple temporally aware model and null*\n",
    "\n",
    "    - a common null model is one in which every forecast for time *t* must have the value at time *t - 1*. If the model does not perform better than a simple model, you cannot justify it. If a simple, naive model manages to outperform the model you created, your model has an intrinsic loss function or data preprocessing problem, rather than a hyperparameter grid search problem. Alternatively, it could be a data signal that has a lot of noise relative to the signal, which also suggests that your model is useless for its intended purpose.\n",
    "    \n",
    "*Study how the model deals with outliers*\n",
    "\n",
    "    - in many areas, outliers are simply data outside the normal curve. These events probably could not be predicted. That is, the best your model can do is ignore these outliers instead of adjusting for them. In fact, if your model predicts outliers well, this could be a sign of overfitting or poor loss function selection. This depends on the model you chose and the loss functions you employed. However, for most uses, a model whose predictions are not as extreme as the extreme values ​​in your data set is recommended. Of course, this recommendation does not apply when the cost of outlier events is high and when the forecasting task is mainly to warn about outlier events when possible.\n",
    "    \n",
    "*Perform a temporal sensitivity analysis*\n",
    "\n",
    "    - are qualitatively similar behaviors in related time series generating related results in your model? When using knowledge of your system's underlying dynamics, make sure it applies and that your model recognizes and treats similar temporal patterns in the same way. For example, if one time series shows an upward trend with a drift of 3 units per day and another shows an upward trend with a drift of 2.9 units per day, you want to make sure that the predictions made for these series are similar. . Furthermore, you would like to be sure that the classification of the predictions and comparison with the input data was sensible (a larger drift should result in a larger prediction value). If this is not the case, your model may be overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f874feb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
