---
title: "applications-in-the-field-of-medical-care"
author: "Della"
date: "2025-02-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Now, we will examine **time series** analysis in the context of healthcare, starting from two case studies: influenza prediction and nowcasting and blood glycemic index prediction. Both uses are important for detecting common health problems. Furthermore, in both cases, these are not solved problems, but rather topics of ongoing research in academia and healthcare.
    
## Predicting the Flu

Predicting the weekly flu rate in a given geographic area is a long-standing and ongoing problem. Infectious disease experts and global security professionals agree that infectious diseases pose a significant risk to human well-being. This is the case with influenza, which affects vulnerable people around the world, causing hundreds of deaths every year, especially among the very young and very old. It is crucial, from both a health care and national security perspective, to develop accurate models of how the flu will develop in a given season. Flu prediction models help predict the virus specifically and also help researchers explore general theories about how infectious diseases travel geographically.

### Case Study on Flu in a Metropolitan Area

We will look at a dataset of weekly flu reports from a variety of administrative regions in France, corresponding to the years 2004 to 2013. We will predict the flu rate in Île-de-France, the Paris metropolitan region. You can download the data from Kaggle (https://perma.cc/W9VQ-UUJC).

#### Data exploration and cleansing

We will start by familiarizing ourselves with the raw data by first analyzing it in its tabular format:

```{r, echo=FALSE}
library(knitr)
library(data.table)

# Sample dataset
flu <- data.table(
  Id = c(3235, 3236, 3237, 3238, 3239, 3240),
  week = rep(201352, 6),
  region_code = c(42, 72, 83, 25, 26, 53),
  region_name = c("ALSACE", "AQUITAINE", "AUVERGNE", "BASSE-NORMANDIE", "BOURGOGNE", "BRETAGNE"),
  TauxGrippe = c(7, 0, 88, 15, 0, 67),
  flu_rate = c(7, 0, 88, 15, 0, 67)
)

# Create a clean table
kable(flu, caption = "Flu Data Table")
```

We also did some basic quality checks, like looking for NA in our variables of interest. We may not know where these NA values come from, but we will need to consider them:

```R
> nrow(flu[is.na(flu.rate)]) / nrow(flu)
[1] 0
> unique(flu[is.na(flu.rate)]$region_name)
character(0)
```

The overall index of NA data points is 0. Furthermore, our region of interest, Île-de-France, is not included in the list of regions with NA values. We did some data cleaning, separating the week and year portion of the timestamp column (which is currently in character format, not numeric or timestamp format):

```R
flu[, year := as.numeric(substr(week, 1, 4))]
flu[, wk   := as.numeric(substr(week, 5, 6))]
```

We will add a *Date* class column so that we can have better plotting axes for time than if we treated the data without a timestamp:

```R
flu[, date := ISOweek2date(paste0(substr(as.character(week), 1, 4), "-W", substr(as.character(week), 5, 6), "-1"))]
```

This line of code is a little complicated. To convert month/week combinations into dates, we add a component that indicates the day. This is the purpose of *paste0()*, which marks each date as the first day of the week, placing a "1" in a string that already designates the year and week (out of the 52 weeks of the year). Note the %U and %u in the format string: they have to do with marking time according to the week of the year and the day of the week, a somewhat unusual timestamp format. We then split the data relating specifically to Paris and sorted it by date:

```{r, echo=FALSE}
library(knitr)
library(data.table)

# Sample dataset
flu <- data.table(
  week = c(200401, 200402, 200403, 200404, 200405, 201348, 201349, 201350, 201351, 201352),
  date = as.Date(c("2003-12-29", "2004-01-05", "2004-01-12", "2004-01-19", "2004-01-26",
                   "2013-11-25", "2013-12-02", "2013-12-09", "2013-12-16", "2013-12-23")),
  flu.rate = c(66, 74, 88, 26, 17, 12, 10, 13, 49, 24)
)

# Create a formatted table
kable(flu, caption = "Flu Data Table")
```

If you've been paying attention, you'll be surprised by the line count. If there are 52 weeks in a year and we have 10 years of data, why do we have 522 rows? We expected 52 weeks x 10 years = 520 lines. Likewise, why are there two NA dates? If we go back to the original data, we have an explanation. Apparently it has a 53˚ for both 2004 and 2009. From time to time there is a year with 53 weeks instead of 52.

Next, we'll check that the data covers a full, regularly sampled date range by first making sure that each year has the same number of data points:

```{r, echo=FALSE}
library(knitr)
library(data.table)

# Sample dataset
paris.flu <- data.table(
  year = c(2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013),
  N = c(53, 52, 52, 52, 52, 53, 52, 52, 52, 52)
)

# Create a formatted table
kable(paris.flu, caption = "Paris Flu Data - Weekly Observations per Year")
```

We can see that the data is as expected; that is, each year (except the two we just looked at) has 52 weeks, and each year-to-week label has 10 data points, one for each year (except week 53). Since we consider the timestamps of the data, we will inspect the actual values of the **time series** (so far we have only considered time indexing). Is there a trend? We will analyze:

![By plotting the flu rate time series, we can see the seasonality of the flu rate](https://github.com/DellaCortef/time-series-pocket-reference/blob/main/applications-field-of-healthcare/images/flu-paris-ts.R.png?raw=true)

A partir de um simples gráfico de linhas, fica claro que há uma sazonalidade considerável (algo que você provavelmente já viu em sua própria comunidade). Esse gráfico sugere um forte componente sazonal, porém não sugere um drift temporal além da sazonalidade.

Things get complicated in the seasonal behavior of the 53rd week. If we want to fit a seasonal model, we need to define seasonality in weeks of the year and we cannot have variable seasonal sizes. While we can imagine some creative solutions to the 53rd week problem, we will choose to exclude this data:

```R
paris.flu <- paris.flu[week != 53]  
```

Whether deleting a data point is a significant problem will depend on the dataset and the question we are asking. Consider this as an exercise: explore other possibilities to adjust the data, keeping data from the 53rd week. There are several options for doing this. One of them is to merge the data from the 53rd week with the data from the 52nd, calculating the average of the two weeks. Another is to use a model that can take cyclical behavior into account, without being stuck with exactly the same cycle length each year. A third option is to use a machine learning model capable of accommodating this with some creative labeling of the data and thus indicating seasonality to the model as an input characteristic.

#### Tuning a seasonal ARIMA model

First, we will consider fitting a seasonal ARIMA model to the data due to strong seasonality. In this case, the periodicity of the data is 52, since the data is sampled weekly. We want to choose a relatively parsimonious model - without too many parameters - because, with 520 data points, our **time series** is not very long.

This **time series** is a good example of how we can go wrong if we rely too much on autopilot. For example, we can think about whether or not we should differentiate the data. Thus, we can consider the autocorrelation graph of the flu index and the autocorrelation of the **time series** differentiated from the flu index. Each is shown in the following figure:

![](https://github.com/DellaCortef/time-series-pocket-reference/blob/main/applications-field-of-healthcare/images/acf-paris-flu-rate.png?raw=true)

![In the first graph, we plot the autocorrelation function for the Paris flu index, and in the second graph, we plot the differentiated Paris flu index. We only analyzed a limited range of lag values.](https://github.com/DellaCortef/time-series-pocket-reference/blob/main/applications-field-of-healthcare/images/acf-diff-paris-flu-rate.png?raw=true)

![](https://github.com/DellaCortef/time-series-pocket-reference/blob/main/applications-field-of-healthcare/images/acf-paris-flu-lag-max.png?raw=true)

![In the first graph, we plot the autocorrelation function for the Paris flu index and, in the second graph, we plot the differentiated Paris flu index. We now examine a wider range of lag values.](https://github.com/DellaCortef/time-series-pocket-reference/blob/main/applications-field-of-healthcare/images/acf-diff-paris-flu-lag-max.png?raw=true)

This gives us a more realistic picture of the autocorrelation of our **time series**. As we can see, there are substantial autocorrelations at various lags, and this makes sense (at least in my experience) in a four-season climate. Flu rates will have a strong correlation with neighboring weeks - that is, close to the time of measurement.

They will also have a strong correlation, given seasonality, with lag periods around 52 or around 104, as this indicates annual seasonality. However, flu rates also have a very strong relationship with lag time periods due to intermediate values, such as half a year (26 weeks), as these lags are also related to seasonal differences and predictable meteorological variations. For example, we know that in half a year the flu value will probably have changed a little. If it was high before, it must now be low and vice versa, again due to seasonality. All of this is represented in the upper graph of the last figures.

Next, we look at the differentiated series, as illustrated in the last graph. Now we see that a considerable part of the **time series** autocorrelation has decreased. However, there is some autocorrelation across a range of values, not only at 52 or 104 weeks (one or two years), but also at intermediate values.

While we may be tempted to continue differentiating, we need to remember that real-world data will never fit perfectly into a SARIMA model. Instead, we will look for a reasonable way to model the data. We can consider seasonal differentiation once again, or adopt a different tactic and differentiate in linear time. We will plot each of these possibilities:

![](https://github.com/DellaCortef/time-series-pocket-reference/blob/main/applications-field-of-healthcare/images/plot-diff-diff-paris-flu-rate-52.png?raw=true)

![Plot two differentiating versions of our series to get an idea of the seasonal behavior of the data](https://github.com/DellaCortef/time-series-pocket-reference/blob/main/applications-field-of-healthcare/images/plot-diff-diff-paris-flu-rate-1.png?raw=true)

Although neither result is ideal, the latter, a first standard differentiation of a seasonal differentiation, is more satisfactory.

The decision to adjust or choose a parameter is a matter of judgment, as is the application of tests. Here, we chose to give weight to the seasonality that we clearly observe, but we also use it because it does not harm the model or make it unduly opaque. This way, we will fit a SARIMA model (*p, d, q*) (*P, D, Q*), with *d* = 1 and *D* = 1. Next, we will choose our AR and MA parameters for our standard ARIMA parameters, *p* and *q*. We do this with standard views, using the following code:

![Partial autocorrelation function graph of the differentiated series we selected.](https://github.com/DellaCortef/time-series-pocket-reference/blob/main/applications-field-of-healthcare/images/acf-pacf-diff-diff-52-lagmax-104.png?raw=true)

We have a limited set of data and prefer a simpler model. The PACF model suggests that an AR(2) model may be appropriate, so we will parsimoniously model our data as a SARIMA(2, 1, 0). We are interested in understanding how this model would work if we continually tweaked it on new data as it became available, the way most models built for the world's systems work. In other words, if we were modeling the flu rate for several years, each week modeling only the data available up to that point, how would our model perform? We’ll answer this as we slide through the model tuning and evaluation, like this:

```R
## Arima adjustment
## Let's estimate 2 weeks 1st time
first.fit.size <- 104
h <- 2
n <- nrow(paris.flu) - h - first.fit.size

## Gets the default dimensions for fits that we will produce
## and related information such as coefficients
first.fit <- arima(paris.flu$flu.rate[1:first.fit.size], order = c(2, 1, 0), 
                   seasonal = list(order = c(0, 1, 0), period = 52))
first.order <- arimaorder(first.fit)

## Pre-allocate space to store our predictions and coefficients
fit.preds <- array(0, dim = c(n, h))
fit.coefs <- array(0, dim = c(n, length(first.fit$coef)))

## After initial adjustment, we advance the adjustment one week at a time, each time 
## readjusting the model and saving the new coefficients and the new prediction
for (i in (first.fit.size + 1):(nrow(paris.flu) - h)) {
  ## predict for an increasingly large window
  data.to.fit = paris.flu[1:i]
  fit = arima(data.to.fit$flu.rate, order = first.order[1:3],
              seasonal = first.order[4:6])
  fit.preds[i - first.fit.size, ] <- forecast(fit, h = 2)$mean
  fit.preds[i - first.fit.size, ] <- fit$coef
}
```

Then we will plot these results:

![Flu rates [flu rate](points) accompanied by our SARIMA forecasts. The predictions of this simple model can assist in public health planning.](https://github.com/DellaCortef/time-series-pocket-reference/blob/main/applications-field-of-healthcare/images/actual-predicted-sarima.png?raw=true)

Now that we have considered fitting a basic ARIMA model to this problem, we will look at other modeling possibilities.

#### Alternative ARIMA model: exogenous harmonic regressors instead of seasonality

Given the performance of the previously analyzed SARIMA model, we can make a variety of modifications. Here, we will consider two modifications, each independent of the other and which can be applied separately.

First, we would like to apply constraints to our model to avoid predicting negative values. One way to do this is to use logarithmic transformation of the data, so that we can predict the logarithm of the **time series** value, rather than the value itself. Thus, when we want to see the "true" series that represents the actually measured numbers, we will use an exponential transformation to backtrack the logarithmic transformation and obtain the predictions in their units.

Second, we would like to find a more transparent way to deal with seasonality in the data. Even though we represented our seasonal ARIMA model as a simple model, in practice a model that deals with a seasonal recurrence of 52 is not simple at all. Seasonal ARIMA models perform best on shorter seasonal cycles and worse on longer seasonal cycles (52 is a long seasonal cycle).

Here, we will use *dynamic harmonic regression*. In this approach, we will find a Fourier series that represents the periodicity in our data and then use that series as an exogenous regressor that fits the ARIMA terms. Because we can extrapolate the Fourier series ahead of time (due to its uniquely periodic nature), we can also pre-calculate the values we expect for the future when generating forecasts. The strength of this model is that its degrees of freedom can be used to explain underlying behavior in addition to seasonal behavior, rather than devoting too much explanatory power to seasonal behavior.

But harmonic regression also has its disadvantages. First, we assume that the behavior is very regular and repeats itself at exactly the same interval. Second, we assume that seasonal behavior is not changing; that is, the period and amplitude of seasonal behavior are not changing. These limitations are similar to the SARIMA model, although the SARIMA model shows more flexibility in how the range of seasonality impacts the data over time. Next, we will demonstrate how to perform a harmonic regression with R similar to the one we used before:

```R
## Pre-allocate vectors to maintain coefficients and adjustments
fit.preds       <- array(0, dim = c(n, h))
fit.coefs       <- array(0, dim = c(n, 100))

## Exogenous regressors that are components of the Fourier series fitted to the data
flu.ts          <- ts(log(paris.flu$flu.rate + 1) + 0.0002, frequency = 52)

## Add small offsets as small values cause problems
exog.regressors <- fourier(flu.ts, K = 2)
exog.colnames   <- colnames(exog.regressors)

## Adjust the model again each week with the expansion window
## of training data
for (i in (first.fit.size + 1):(nrow(paris.flu) - h)) {
  data.to.fit       <- ts(flu.ts[1:i], frequency = 52)
  exogs.for.fit     <- exog.regressors[1:i,]
  exogs.for.predict <- exog.regressors[(i + 1):(i + h),]
  
  fit <- auto.arima(data.to.fit,
                    xreg = exogs.for.fit,
                    seasonal = FALSE)
  
  fit.preds[i - first.fit.size, ] <- forecast(fit, h = h,
                                              xreg = exogs.for.predict)$mean
  fit.coefs[i - first.fit.size, 1:length(fit$coef)] = fit$coef 
}
```

Here we made some adjustments to the previous code. First, we use a *ts* object. With an object, we explicitly indicate the seasonality of the **time series** (52 weeks) when creating it. We also performed a logarithmic transformation of the data at this point to ensure positive predictions of our final value of interest, the flu index:

```R
flu.ts = ts(log(paris.flu$flu.rate + 1) + 0.0001,
            frequency = 52)
```

We also added a small numerical offset (+ 0.0001), because numerical adjustment does not work well with strict zero values or even very small values. One of our two adjustments has already been performed with just this line of code (i.e., the logarithmic transformation to apply the physical condition of non-negative values). We then generate the exogenous harmonic regressors (the Fourir approximation) that we are using in place of the seasonal parameters in SARIMA. We do this using the *fourir()* function from the **forecast** package:
```R
exog.regressors <- fourier(flu.ts, K = 2)
exog.colnames   <- colnames(exog.regressors)
```

We first generate the harmonic series that accompanies our entire **time series**, and then subdivide it as appropriate for our later expanding window adjustment in the loop.

The *K* hyperparameter signals how many separate sine/cosine pairs we will include in our fit, where each represents a new frequency used to tune the sine/cosine. In general, *K* will be higher for longer seasonal periods and lower for shorter periods. In a more complete example, we could consider setting an information criterion to adjust *K*, but in this example we use *K* = 2 as a reasonable model.

Finally, all that's left to do is generate new fits that take into account the exogenous Fourir components that we just adjusted. We do the adjustment as follows: the *xreg* parameter contains the Fourier adjustment series as additional regressors, which are adjusted together with the standard ARIMA parameters:

```R
fit <- auto.arima(data.to.fit, xreg = exogs.for.fit, seasonal = FALSE)
```

We set the *seasonal* parameter to FALSE in order to ensure that we do not have redundant seasonal parameters, given our decision to use dynamic harmonic regression. It is also necessary to include regressors when we generate a forecast, which means that we need to indicate what the regressors will be at the moment the forecast is segmenting:

```R
  fit.preds[i - first.fit.size, ] <- forecast(fit, h = h,
                                              xreg = exogs.for.predict)$mean
```

We plot the performance of this model as follows:

![Plot of actual flu rates (points) compared to our predictions from the ARIMA dynamic harmonic regression model](https://github.com/DellaCortef/time-series-pocket-reference/blob/main/applications-field-of-healthcare/images/arima-harmoni-regressors-flu-prediction.png?raw=true)

The advantage is that now our predicted series no longer has negative values. However, there are many disappointing aspects of the model's performance. The most obvious is that many of the predictions are bad. The peaks have magnitude and occur at the wrong time.

One explanation for the problems is that regular seasonality is not a good representation of flu seasonality. For example, the flu can peak each winter in early December or late March. We can observe this in the test data. Consider following the following code and graph, which identifies the peaks in the test range of the data:

![Graph that only includes test values for influenza and their apparent peak locations.](https://github.com/DellaCortef/time-series-pocket-reference/blob/main/applications-field-of-healthcare/images/test-flu-rate.png?raw=true)

With peak-to-peak distances sampled from over 984 in one case and 50 in another, we can see a lot of variability from year to year. Our dynamic harmonic regression would not consider this and would apply a stricter seasonality model than the SARIMA model, since the latter's seasonal behavior can change over time. This mismatch between the assumptions and the data may go a long way toward explaining the poor performance of this model, so using a bad model actually drew our attention to important features of our data that we had not noticed before.

Despite its low performance, this alternative model proved useful for a few reasons. He showed us the value of including the physical limitations of the system in our data preprocessing, in this case through a logarithmic transformation. And it also showed us the value of testing multiple classes of models with an underlying theory behind model selection at each point. We chose this model to simplify our seasonality, but what we discovered is that probably neither SARIMA nor the seasonality dynamic harmonic regression model are good for this system.

### What is the Cutting-edge Technology in Flu Prediction?

The models we just explored were relatively simple, yet reasonably effective in making immediate or non-immediate predictions of influenza rates in the Paris metropolitan region for short forecast horizons. These models are a good start for understanding your data set and recognizing the limitations of what could be predictive in a noisy but complicated system like the flu rate in a given geographic region.