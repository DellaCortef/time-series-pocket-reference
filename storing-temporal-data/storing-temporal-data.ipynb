{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bde6d25f",
   "metadata": {},
   "source": [
    "# Storing Temporal Data\n",
    "In general, the value of **time series** data is in its retrospective (batch ingestion model), rather than in the live streaming of data. For this reason, storing **time series** data is necessary for most analyses.\n",
    "\n",
    "A good storage solution is one that allows for easy access and reliability of data without requiring a large investment of computing resources. Later, we will look at what aspects of a dataset we should consider for storage, as well as examine the advantages of SQL databases, NoSQL databases, and a variety of flat file formats.\n",
    "\n",
    "Developing a general **time series** storage solution is challenging because there are many different types of data, each with different storage, read/write, and analysis patterns. Some data will be stored and examined repeatedly, while others are only useful for a short period of time, after which they may be deleted entirely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007e233c",
   "metadata": {},
   "source": [
    "Use case examples:\n",
    "\n",
    "<ins>*Use case 1˚:*</ins>\n",
    "- We are collecting performance metrics on a production system. These performance metrics need to be stored for years on end, but the older the data gets, the less detailed it needs to be. Therefore, a storage medium is needed that automatically performs *downsampling* and separates the data as the information becomes old;\n",
    "\n",
    "<ins>*Use case 2˚:*</ins>\n",
    "- We have remote access to an open source repository of **time series** data, but we need to keep a local copy on your computer to reduce network traffic. The remote repository stores each time series in a folder of downloadable files on a web server, but we would like to compile all of these files into a single database to simplify things. The data must be immutable and capable of being stored indefinitely, as the aim is to have a reliable copy of the remote repository;\n",
    "\n",
    "<ins>*Use case 3˚:*</ins>\n",
    "- We create our own **time series** data by integrating a variety of data sources at different time scales, and with distinct pre-processing and formatting. Data collection and processing were tiring and time-consuming. We would like to store the data in its final format instead of running a pre-processing step successively, but we would also like to keep the raw data, to later explore pre-processing alternatives. You may need to re-examine the processed and raw data frequently as you develop new machine learning models, refitting new models on the same data, and also adding data over time as newer raw data becomes available. No need to downsample or separate data in storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62f9356",
   "metadata": {},
   "source": [
    "Use cases solutions:\n",
    "\n",
    "<ins>*Importance of how performance scales with size*</ins>\n",
    "- in the first use case, we would look for a solution that could incorporate automated scripts to delete old data. We wouldn't be concerned about how the system scales to large datasets, as we plan to keep the dataset small. For the second and third case, we would expect to have a large and stable collection of data or a large and growing collection of data, respectively;\n",
    "\n",
    "<ins>*Importance of random access versus sequential access of data points*</ins>\n",
    "- in the second case, we expect all data to be accessed in equal parts, since this **time series** data would all have the same \"age\" upon insertion and would all reference the relevant data set. In contrast, in the first and third cases, we expect the most recent data to be accessed more frequently;\n",
    "\n",
    "<ins>*Importance of automation scripts*</ins>\n",
    "- apparently, the first case can be automated, while the second case would not require automation (since the data would be immutable). The third case suggests little automation, and also a considerable amount of data collection and processing of all parts of the data, not just the most recent ones. In the first case, we want a storage solution that can be integrated with scripts or stored procedures, while in the third case we want a solution that allows easy customization of data processing;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87ff793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
